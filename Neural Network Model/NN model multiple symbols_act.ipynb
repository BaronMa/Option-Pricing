{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-19.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Requirement already satisfied: h5py in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.14.3)\n",
      "Requirement already satisfied: pyyaml in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (3.12)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/4a/5c86ed8b245aa48f9f819b13a0a9039e9126ba19fdd0c7e0b8026c12315a/tensorflow-1.14.0-cp36-cp36m-win_amd64.whl (68.3MB)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/59/ec6c6075dd628f2efd33dbe11b259e63e1de7cbc26e45b38de767df528d4/protobuf-3.8.0-cp36-cp36m-win_amd64.whl (1.1MB)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.0.8)\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/ed/e036d31a9b2c750f270cbb1cfc1c0f94ac78ae504eea7eec3267be4e294a/numpy-1.16.4-cp36-cp36m-win_amd64.whl (11.9MB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/6a/e83233ed636bdf8668f0e79897fd70bce04869482dd88f3cfc4c42404fb2/grpcio-1.21.1-cp36-cp36m-win_amd64.whl (1.6MB)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.1)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: h5py in c:\\program files\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.1)\n",
      "Building wheels for collected packages: gast, absl-py, termcolor, wrapt\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\ee\\98\\38\\46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\d7\\de\\2e\\efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "Successfully built gast absl-py termcolor wrapt\n",
      "Installing collected packages: gast, protobuf, numpy, absl-py, grpcio, markdown, tensorboard, astor, google-pasta, termcolor, wrapt, tensorflow-estimator, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.21.1 markdown-3.1.1 numpy-1.16.4 protobuf-3.8.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 wrapt-1.11.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorboard 1.14.0 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts freeze_graph.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user keras\n",
    "!pip install --user tensorflow\n",
    "!pip install --user -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU only\n",
    "# Only Macbook needs to run this cell\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and do data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>exdate</th>\n",
       "      <th>maturity</th>\n",
       "      <th>strike_price</th>\n",
       "      <th>best_offer</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>underlying_price</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>cp_flag_C</th>\n",
       "      <th>cp_flag_P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.181376</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>62.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.450289</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>67.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.369425</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.888123</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date      exdate  maturity  strike_price  best_offer  \\\n",
       "0    AXP  2013-01-02  2013-01-04         2          60.0        0.03   \n",
       "1    AXP  2013-01-02  2013-01-04         2          62.5        0.05   \n",
       "2    AXP  2013-01-02  2013-01-04         2          65.0        0.05   \n",
       "3    AXP  2013-01-02  2013-01-04         2          67.5        0.50   \n",
       "4    AXP  2013-01-02  2013-01-04         2          70.0        0.01   \n",
       "\n",
       "   impl_volatility  underlying_price  interest_rate  cp_flag_C  cp_flag_P  \n",
       "0         0.181376             58.75         0.0008          1          0  \n",
       "1         0.450289             58.75         0.0008          1          0  \n",
       "2         0.676564             58.75         0.0008          1          0  \n",
       "3         1.369425             58.75         0.0008          1          0  \n",
       "4         0.888123             58.75         0.0008          1          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Options.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['interest_rate'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['best_offer'].values\n",
    "X = df[['maturity', 'strike_price', 'impl_volatility', 'underlying_price', 'cp_flag_C', 'cp_flag_P', 'interest_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to build a regression neural network model\n",
    "#sc= preprocessing.MinMaxScaler()\n",
    "#X= sc.fit_transform(X)\n",
    "X = preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1559488, 7)\n",
      "(1559488,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train, XX_validation, yy_train, yy_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a keras Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 21:05:21.055255 10472 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0705 21:05:21.071266 10472 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0705 21:05:21.077271 10472 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Built Keras sequential model with 3 hidden layer, and after the first hidden layer.\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable early stopping based on the loss of validation data\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because it is a simple regression problem, we should custom metrics function\n",
    "from keras import backend as K\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis = -1))\n",
    "\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 21:05:31.662904 10472 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use mean_squared_error to compile regression model loss\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 21:05:40.826040 10472 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0705 21:05:40.982231 10472 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1247590 samples, validate on 311898 samples\n",
      "Epoch 1/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 74.4599 - rmse: 4.4000 - r_square: 0.6299 - val_loss: 7.1266 - val_rmse: 1.4774 - val_r_square: 0.9638\n",
      "Epoch 2/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 4.3325 - rmse: 1.1099 - r_square: 0.9780 - val_loss: 3.0192 - val_rmse: 0.9518 - val_r_square: 0.9848\n",
      "Epoch 3/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 2.1587 - rmse: 0.7984 - r_square: 0.9890 - val_loss: 1.2478 - val_rmse: 0.6063 - val_r_square: 0.9937\n",
      "Epoch 4/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 1.6375 - rmse: 0.6992 - r_square: 0.9917 - val_loss: 0.8913 - val_rmse: 0.5652 - val_r_square: 0.9955\n",
      "Epoch 5/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 1.3896 - rmse: 0.6384 - r_square: 0.9929 - val_loss: 0.9275 - val_rmse: 0.5629 - val_r_square: 0.9953\n",
      "Epoch 6/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 1.2914 - rmse: 0.6132 - r_square: 0.9934 - val_loss: 0.7112 - val_rmse: 0.4696 - val_r_square: 0.9964\n",
      "Epoch 7/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 1.2673 - rmse: 0.5946 - r_square: 0.9936 - val_loss: 0.7589 - val_rmse: 0.5125 - val_r_square: 0.9961\n",
      "Epoch 8/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 1.1042 - rmse: 0.5642 - r_square: 0.9944 - val_loss: 0.9474 - val_rmse: 0.5179 - val_r_square: 0.9952\n",
      "Epoch 9/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 1.0146 - rmse: 0.5436 - r_square: 0.9948 - val_loss: 0.6383 - val_rmse: 0.4457 - val_r_square: 0.9968\n",
      "Epoch 10/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.8997 - rmse: 0.5183 - r_square: 0.9954 - val_loss: 0.4835 - val_rmse: 0.4090 - val_r_square: 0.9975\n",
      "Epoch 11/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.8758 - rmse: 0.5133 - r_square: 0.9955 - val_loss: 0.8634 - val_rmse: 0.4751 - val_r_square: 0.9956\n",
      "Epoch 12/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.8639 - rmse: 0.5039 - r_square: 0.9956 - val_loss: 0.6044 - val_rmse: 0.4581 - val_r_square: 0.9969\n",
      "Epoch 13/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.7533 - rmse: 0.4798 - r_square: 0.9962 - val_loss: 0.6617 - val_rmse: 0.4938 - val_r_square: 0.9966\n",
      "Epoch 14/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.7922 - rmse: 0.4813 - r_square: 0.9959 - val_loss: 1.7516 - val_rmse: 0.7670 - val_r_square: 0.9912\n",
      "Epoch 15/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.6706 - rmse: 0.4517 - r_square: 0.9966 - val_loss: 1.0445 - val_rmse: 0.7158 - val_r_square: 0.9947\n",
      "Epoch 16/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.7120 - rmse: 0.4563 - r_square: 0.9963 - val_loss: 1.0940 - val_rmse: 0.5959 - val_r_square: 0.9945\n",
      "Epoch 17/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.6460 - rmse: 0.4399 - r_square: 0.9967 - val_loss: 0.9003 - val_rmse: 0.4956 - val_r_square: 0.9954\n",
      "Epoch 18/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.6064 - rmse: 0.4265 - r_square: 0.9969 - val_loss: 0.7533 - val_rmse: 0.4726 - val_r_square: 0.9962\n",
      "Epoch 19/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.6214 - rmse: 0.4273 - r_square: 0.9968 - val_loss: 0.7265 - val_rmse: 0.4284 - val_r_square: 0.9963\n",
      "Epoch 20/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.5557 - rmse: 0.4073 - r_square: 0.9972 - val_loss: 0.3289 - val_rmse: 0.3375 - val_r_square: 0.9983\n",
      "Epoch 21/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.5514 - rmse: 0.4039 - r_square: 0.9972 - val_loss: 0.3908 - val_rmse: 0.3490 - val_r_square: 0.9980\n",
      "Epoch 22/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.5354 - rmse: 0.3986 - r_square: 0.9973 - val_loss: 0.3125 - val_rmse: 0.3197 - val_r_square: 0.9984\n",
      "Epoch 23/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.5271 - rmse: 0.3921 - r_square: 0.9973 - val_loss: 0.3459 - val_rmse: 0.3342 - val_r_square: 0.9982\n",
      "Epoch 24/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4920 - rmse: 0.3802 - r_square: 0.9975 - val_loss: 0.4156 - val_rmse: 0.3467 - val_r_square: 0.9979\n",
      "Epoch 25/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4629 - rmse: 0.3703 - r_square: 0.9976 - val_loss: 1.3208 - val_rmse: 0.5338 - val_r_square: 0.9934\n",
      "Epoch 26/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4887 - rmse: 0.3744 - r_square: 0.9975 - val_loss: 0.2501 - val_rmse: 0.2872 - val_r_square: 0.9987\n",
      "Epoch 27/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4801 - rmse: 0.3671 - r_square: 0.9976 - val_loss: 0.5954 - val_rmse: 0.4443 - val_r_square: 0.9970\n",
      "Epoch 28/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4646 - rmse: 0.3628 - r_square: 0.9976 - val_loss: 0.3658 - val_rmse: 0.3318 - val_r_square: 0.9981\n",
      "Epoch 29/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4112 - rmse: 0.3480 - r_square: 0.9979 - val_loss: 1.1220 - val_rmse: 0.5224 - val_r_square: 0.9943\n",
      "Epoch 30/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4352 - rmse: 0.3525 - r_square: 0.9978 - val_loss: 0.2331 - val_rmse: 0.2909 - val_r_square: 0.9988\n",
      "Epoch 31/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4090 - rmse: 0.3418 - r_square: 0.9979 - val_loss: 0.4860 - val_rmse: 0.4181 - val_r_square: 0.9975\n",
      "Epoch 32/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3909 - rmse: 0.3385 - r_square: 0.9980 - val_loss: 0.3696 - val_rmse: 0.3944 - val_r_square: 0.9981\n",
      "Epoch 33/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.4110 - rmse: 0.3396 - r_square: 0.9979 - val_loss: 0.2101 - val_rmse: 0.2611 - val_r_square: 0.9989\n",
      "Epoch 34/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3783 - rmse: 0.3288 - r_square: 0.9981 - val_loss: 0.2679 - val_rmse: 0.3103 - val_r_square: 0.9986\n",
      "Epoch 35/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3662 - rmse: 0.3252 - r_square: 0.9981 - val_loss: 0.2453 - val_rmse: 0.2702 - val_r_square: 0.9987\n",
      "Epoch 36/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3768 - rmse: 0.3276 - r_square: 0.9981 - val_loss: 0.3592 - val_rmse: 0.3293 - val_r_square: 0.9982\n",
      "Epoch 37/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3540 - rmse: 0.3177 - r_square: 0.9982 - val_loss: 0.3346 - val_rmse: 0.3124 - val_r_square: 0.9983\n",
      "Epoch 38/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3452 - rmse: 0.3147 - r_square: 0.9982 - val_loss: 0.4425 - val_rmse: 0.3853 - val_r_square: 0.9977\n",
      "Epoch 39/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3350 - rmse: 0.3105 - r_square: 0.9983 - val_loss: 0.1767 - val_rmse: 0.2379 - val_r_square: 0.9991\n",
      "Epoch 40/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3262 - rmse: 0.3089 - r_square: 0.9983 - val_loss: 0.2508 - val_rmse: 0.2926 - val_r_square: 0.9987\n",
      "Epoch 41/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3083 - rmse: 0.3007 - r_square: 0.9984 - val_loss: 0.1907 - val_rmse: 0.2486 - val_r_square: 0.9990\n",
      "Epoch 42/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3101 - rmse: 0.3011 - r_square: 0.9984 - val_loss: 0.4052 - val_rmse: 0.3539 - val_r_square: 0.9979\n",
      "Epoch 43/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3243 - rmse: 0.3043 - r_square: 0.9983 - val_loss: 0.1803 - val_rmse: 0.2350 - val_r_square: 0.9991\n",
      "Epoch 44/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2956 - rmse: 0.2944 - r_square: 0.9985 - val_loss: 1.1319 - val_rmse: 0.5962 - val_r_square: 0.9943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2989 - rmse: 0.2953 - r_square: 0.9985 - val_loss: 0.2097 - val_rmse: 0.2474 - val_r_square: 0.9989\n",
      "Epoch 46/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2791 - rmse: 0.2886 - r_square: 0.9986 - val_loss: 0.1670 - val_rmse: 0.2361 - val_r_square: 0.9991\n",
      "Epoch 47/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2897 - rmse: 0.2910 - r_square: 0.9985 - val_loss: 0.5616 - val_rmse: 0.4194 - val_r_square: 0.9972\n",
      "Epoch 48/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2747 - rmse: 0.2851 - r_square: 0.9986 - val_loss: 0.1996 - val_rmse: 0.2568 - val_r_square: 0.9990\n",
      "Epoch 49/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2855 - rmse: 0.2866 - r_square: 0.9986 - val_loss: 0.1944 - val_rmse: 0.2417 - val_r_square: 0.9990\n",
      "Epoch 50/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2567 - rmse: 0.2767 - r_square: 0.9987 - val_loss: 1.2126 - val_rmse: 0.5693 - val_r_square: 0.9939\n",
      "Epoch 51/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2562 - rmse: 0.2768 - r_square: 0.9987 - val_loss: 0.3314 - val_rmse: 0.2978 - val_r_square: 0.9983\n",
      "Epoch 52/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2647 - rmse: 0.2787 - r_square: 0.9986 - val_loss: 0.2006 - val_rmse: 0.2499 - val_r_square: 0.9990\n",
      "Epoch 53/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2434 - rmse: 0.2704 - r_square: 0.9988 - val_loss: 0.3915 - val_rmse: 0.3330 - val_r_square: 0.9980\n",
      "Epoch 54/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2519 - rmse: 0.2728 - r_square: 0.9987 - val_loss: 0.2019 - val_rmse: 0.2476 - val_r_square: 0.9990\n",
      "Epoch 55/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2691 - rmse: 0.2753 - r_square: 0.9986 - val_loss: 0.1851 - val_rmse: 0.2425 - val_r_square: 0.9990\n",
      "Epoch 56/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2367 - rmse: 0.2659 - r_square: 0.9988 - val_loss: 0.1485 - val_rmse: 0.2176 - val_r_square: 0.9992\n",
      "Epoch 57/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2324 - rmse: 0.2647 - r_square: 0.9988 - val_loss: 0.2743 - val_rmse: 0.2754 - val_r_square: 0.9986\n",
      "Epoch 58/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2380 - rmse: 0.2647 - r_square: 0.9988 - val_loss: 0.1491 - val_rmse: 0.2234 - val_r_square: 0.9992\n",
      "Epoch 59/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2368 - rmse: 0.2643 - r_square: 0.9988 - val_loss: 0.2036 - val_rmse: 0.2470 - val_r_square: 0.9990\n",
      "Epoch 60/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2377 - rmse: 0.2651 - r_square: 0.9988 - val_loss: 0.1486 - val_rmse: 0.2195 - val_r_square: 0.9992\n",
      "Epoch 61/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2250 - rmse: 0.2584 - r_square: 0.9989 - val_loss: 0.2458 - val_rmse: 0.2929 - val_r_square: 0.9987\n",
      "Epoch 62/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2298 - rmse: 0.2578 - r_square: 0.9988 - val_loss: 0.5363 - val_rmse: 0.4213 - val_r_square: 0.9973\n",
      "Epoch 63/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2214 - rmse: 0.2578 - r_square: 0.9989 - val_loss: 0.1423 - val_rmse: 0.2212 - val_r_square: 0.9993\n",
      "Epoch 64/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2273 - rmse: 0.2572 - r_square: 0.9988 - val_loss: 0.1763 - val_rmse: 0.2506 - val_r_square: 0.9991\n",
      "Epoch 65/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2231 - rmse: 0.2568 - r_square: 0.9989 - val_loss: 0.1309 - val_rmse: 0.2119 - val_r_square: 0.9993\n",
      "Epoch 66/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2155 - rmse: 0.2537 - r_square: 0.9989 - val_loss: 0.1822 - val_rmse: 0.2493 - val_r_square: 0.9991\n",
      "Epoch 67/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2193 - rmse: 0.2555 - r_square: 0.9989 - val_loss: 0.1426 - val_rmse: 0.2137 - val_r_square: 0.9993\n",
      "Epoch 68/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2298 - rmse: 0.2559 - r_square: 0.9988 - val_loss: 0.1508 - val_rmse: 0.2323 - val_r_square: 0.9992\n",
      "Epoch 69/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2210 - rmse: 0.2527 - r_square: 0.9989 - val_loss: 0.1444 - val_rmse: 0.2097 - val_r_square: 0.9993\n",
      "Epoch 70/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2200 - rmse: 0.2535 - r_square: 0.9989 - val_loss: 0.2720 - val_rmse: 0.2766 - val_r_square: 0.9986\n",
      "Epoch 71/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2038 - rmse: 0.2472 - r_square: 0.9990 - val_loss: 0.1434 - val_rmse: 0.2173 - val_r_square: 0.9993\n",
      "Epoch 72/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2112 - rmse: 0.2515 - r_square: 0.9989 - val_loss: 0.1310 - val_rmse: 0.2074 - val_r_square: 0.9993\n",
      "Epoch 73/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2101 - rmse: 0.2499 - r_square: 0.9989 - val_loss: 0.1641 - val_rmse: 0.2262 - val_r_square: 0.9992\n",
      "Epoch 74/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2065 - rmse: 0.2476 - r_square: 0.9990 - val_loss: 0.1528 - val_rmse: 0.2418 - val_r_square: 0.9992\n",
      "Epoch 75/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2049 - rmse: 0.2479 - r_square: 0.9990 - val_loss: 0.1775 - val_rmse: 0.2383 - val_r_square: 0.9991\n",
      "Epoch 76/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2037 - rmse: 0.2452 - r_square: 0.9990 - val_loss: 0.3828 - val_rmse: 0.3271 - val_r_square: 0.9981\n",
      "Epoch 77/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2004 - rmse: 0.2456 - r_square: 0.9990 - val_loss: 0.2084 - val_rmse: 0.2585 - val_r_square: 0.9989\n",
      "Epoch 78/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2021 - rmse: 0.2464 - r_square: 0.9990 - val_loss: 0.1346 - val_rmse: 0.2076 - val_r_square: 0.9993\n",
      "Epoch 79/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1971 - rmse: 0.2428 - r_square: 0.9990 - val_loss: 0.4136 - val_rmse: 0.3093 - val_r_square: 0.9979\n",
      "Epoch 80/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2138 - rmse: 0.2455 - r_square: 0.9989 - val_loss: 0.2171 - val_rmse: 0.2454 - val_r_square: 0.9989\n",
      "Epoch 81/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1886 - rmse: 0.2397 - r_square: 0.9990 - val_loss: 0.1762 - val_rmse: 0.2475 - val_r_square: 0.9991\n",
      "Epoch 82/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1896 - rmse: 0.2400 - r_square: 0.9990 - val_loss: 0.2537 - val_rmse: 0.2712 - val_r_square: 0.9987\n",
      "Epoch 83/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1967 - rmse: 0.2429 - r_square: 0.9990 - val_loss: 0.1356 - val_rmse: 0.2045 - val_r_square: 0.9993\n",
      "Epoch 84/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1933 - rmse: 0.2402 - r_square: 0.9990 - val_loss: 0.1426 - val_rmse: 0.2167 - val_r_square: 0.9993\n",
      "Epoch 85/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1872 - rmse: 0.2383 - r_square: 0.9990 - val_loss: 0.1648 - val_rmse: 0.2262 - val_r_square: 0.9992\n",
      "Epoch 86/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1900 - rmse: 0.2401 - r_square: 0.9990 - val_loss: 0.1358 - val_rmse: 0.2156 - val_r_square: 0.9993\n",
      "Epoch 87/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1943 - rmse: 0.2399 - r_square: 0.9990 - val_loss: 0.1506 - val_rmse: 0.2310 - val_r_square: 0.9992\n",
      "Epoch 88/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1970 - rmse: 0.2405 - r_square: 0.9990 - val_loss: 0.1678 - val_rmse: 0.2280 - val_r_square: 0.9991\n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1827 - rmse: 0.2363 - r_square: 0.9991 - val_loss: 0.1286 - val_rmse: 0.2053 - val_r_square: 0.9993\n",
      "Epoch 90/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1902 - rmse: 0.2380 - r_square: 0.9990 - val_loss: 0.2176 - val_rmse: 0.2578 - val_r_square: 0.9989\n",
      "Epoch 91/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1830 - rmse: 0.2358 - r_square: 0.9991 - val_loss: 0.1249 - val_rmse: 0.2114 - val_r_square: 0.9994\n",
      "Epoch 92/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1824 - rmse: 0.2351 - r_square: 0.9991 - val_loss: 0.2041 - val_rmse: 0.2428 - val_r_square: 0.9990\n",
      "Epoch 93/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1940 - rmse: 0.2394 - r_square: 0.9990 - val_loss: 0.3535 - val_rmse: 0.2951 - val_r_square: 0.9982\n",
      "Epoch 94/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1767 - rmse: 0.2319 - r_square: 0.9991 - val_loss: 0.1668 - val_rmse: 0.2413 - val_r_square: 0.9991\n",
      "Epoch 95/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1876 - rmse: 0.2364 - r_square: 0.9990 - val_loss: 0.1678 - val_rmse: 0.2359 - val_r_square: 0.9991\n",
      "Epoch 96/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1846 - rmse: 0.2364 - r_square: 0.9991 - val_loss: 0.1381 - val_rmse: 0.2127 - val_r_square: 0.9993\n",
      "Epoch 97/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1878 - rmse: 0.2342 - r_square: 0.9990 - val_loss: 0.1710 - val_rmse: 0.2221 - val_r_square: 0.9991\n",
      "Epoch 98/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1835 - rmse: 0.2326 - r_square: 0.9991 - val_loss: 0.1468 - val_rmse: 0.2187 - val_r_square: 0.9993\n",
      "Epoch 99/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1810 - rmse: 0.2339 - r_square: 0.9991 - val_loss: 0.4018 - val_rmse: 0.3405 - val_r_square: 0.9980\n",
      "Epoch 100/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1832 - rmse: 0.2310 - r_square: 0.9991 - val_loss: 0.1776 - val_rmse: 0.2320 - val_r_square: 0.9991\n",
      "Epoch 101/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1723 - rmse: 0.2292 - r_square: 0.9991 - val_loss: 0.3412 - val_rmse: 0.3649 - val_r_square: 0.9983\n",
      "Epoch 102/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1829 - rmse: 0.2343 - r_square: 0.9991 - val_loss: 0.1723 - val_rmse: 0.2289 - val_r_square: 0.9991\n",
      "Epoch 103/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1722 - rmse: 0.2300 - r_square: 0.9991 - val_loss: 0.1166 - val_rmse: 0.1940 - val_r_square: 0.9994\n",
      "Epoch 104/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1665 - rmse: 0.2270 - r_square: 0.9991 - val_loss: 0.1628 - val_rmse: 0.2328 - val_r_square: 0.9992\n",
      "Epoch 105/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1774 - rmse: 0.2317 - r_square: 0.9991 - val_loss: 0.1335 - val_rmse: 0.2077 - val_r_square: 0.9993\n",
      "Epoch 106/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1901 - rmse: 0.2308 - r_square: 0.9990 - val_loss: 0.1985 - val_rmse: 0.2348 - val_r_square: 0.9990\n",
      "Epoch 107/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1771 - rmse: 0.2309 - r_square: 0.9991 - val_loss: 0.1628 - val_rmse: 0.2389 - val_r_square: 0.9992\n",
      "Epoch 108/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1766 - rmse: 0.2293 - r_square: 0.9991 - val_loss: 0.1257 - val_rmse: 0.2030 - val_r_square: 0.9994\n",
      "Epoch 109/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1765 - rmse: 0.2305 - r_square: 0.9991 - val_loss: 0.1078 - val_rmse: 0.1899 - val_r_square: 0.9994\n",
      "Epoch 110/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1732 - rmse: 0.2288 - r_square: 0.9991 - val_loss: 0.1245 - val_rmse: 0.1987 - val_r_square: 0.9994\n",
      "Epoch 111/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1639 - rmse: 0.2247 - r_square: 0.9992 - val_loss: 0.1825 - val_rmse: 0.2338 - val_r_square: 0.9991\n",
      "Epoch 112/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1663 - rmse: 0.2256 - r_square: 0.9991 - val_loss: 0.1228 - val_rmse: 0.1929 - val_r_square: 0.9994\n",
      "Epoch 113/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1673 - rmse: 0.2258 - r_square: 0.9991 - val_loss: 0.1559 - val_rmse: 0.2171 - val_r_square: 0.9992\n",
      "Epoch 114/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1658 - rmse: 0.2255 - r_square: 0.9992 - val_loss: 0.2425 - val_rmse: 0.2756 - val_r_square: 0.9988\n",
      "Epoch 115/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1668 - rmse: 0.2252 - r_square: 0.9991 - val_loss: 0.1182 - val_rmse: 0.2049 - val_r_square: 0.9994\n",
      "Epoch 116/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1670 - rmse: 0.2253 - r_square: 0.9991 - val_loss: 0.1220 - val_rmse: 0.2015 - val_r_square: 0.9994\n",
      "Epoch 117/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1644 - rmse: 0.2241 - r_square: 0.9992 - val_loss: 0.1076 - val_rmse: 0.1928 - val_r_square: 0.9994\n",
      "Epoch 118/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1591 - rmse: 0.2209 - r_square: 0.9992 - val_loss: 0.1169 - val_rmse: 0.1935 - val_r_square: 0.9994\n",
      "Epoch 119/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1685 - rmse: 0.2244 - r_square: 0.9991 - val_loss: 0.1810 - val_rmse: 0.2387 - val_r_square: 0.9991\n",
      "Epoch 120/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1603 - rmse: 0.2219 - r_square: 0.9992 - val_loss: 0.1149 - val_rmse: 0.1953 - val_r_square: 0.9994\n",
      "Epoch 121/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1614 - rmse: 0.2214 - r_square: 0.9992 - val_loss: 0.2695 - val_rmse: 0.3152 - val_r_square: 0.9986\n",
      "Epoch 122/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1613 - rmse: 0.2210 - r_square: 0.9992 - val_loss: 0.1103 - val_rmse: 0.1851 - val_r_square: 0.9994\n",
      "Epoch 123/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1641 - rmse: 0.2226 - r_square: 0.9992 - val_loss: 0.2119 - val_rmse: 0.2688 - val_r_square: 0.9989\n",
      "Epoch 124/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1628 - rmse: 0.2216 - r_square: 0.9992 - val_loss: 0.2241 - val_rmse: 0.2535 - val_r_square: 0.9989\n",
      "Epoch 125/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1669 - rmse: 0.2245 - r_square: 0.9991 - val_loss: 0.1310 - val_rmse: 0.2099 - val_r_square: 0.9993\n",
      "Epoch 126/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1573 - rmse: 0.2191 - r_square: 0.9992 - val_loss: 0.1211 - val_rmse: 0.1920 - val_r_square: 0.9994\n",
      "Epoch 127/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1599 - rmse: 0.2185 - r_square: 0.9992 - val_loss: 0.1040 - val_rmse: 0.1866 - val_r_square: 0.9995\n",
      "Epoch 128/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1583 - rmse: 0.2196 - r_square: 0.9992 - val_loss: 0.1897 - val_rmse: 0.2537 - val_r_square: 0.9990\n",
      "Epoch 129/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1535 - rmse: 0.2179 - r_square: 0.9992 - val_loss: 0.1103 - val_rmse: 0.2011 - val_r_square: 0.9994\n",
      "Epoch 130/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1622 - rmse: 0.2194 - r_square: 0.9992 - val_loss: 0.2328 - val_rmse: 0.2595 - val_r_square: 0.9988\n",
      "Epoch 131/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1586 - rmse: 0.2199 - r_square: 0.9992 - val_loss: 0.1222 - val_rmse: 0.2058 - val_r_square: 0.9994\n",
      "Epoch 132/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1510 - rmse: 0.2159 - r_square: 0.9992 - val_loss: 0.1761 - val_rmse: 0.2336 - val_r_square: 0.9991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1562 - rmse: 0.2176 - r_square: 0.9992 - val_loss: 0.1813 - val_rmse: 0.2347 - val_r_square: 0.9991\n",
      "Epoch 134/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1526 - rmse: 0.2158 - r_square: 0.9992 - val_loss: 0.1561 - val_rmse: 0.2079 - val_r_square: 0.9992\n",
      "Epoch 135/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1551 - rmse: 0.2181 - r_square: 0.9992 - val_loss: 0.1216 - val_rmse: 0.2063 - val_r_square: 0.9994\n",
      "Epoch 136/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1553 - rmse: 0.2168 - r_square: 0.9992 - val_loss: 0.2331 - val_rmse: 0.2429 - val_r_square: 0.9988\n",
      "Epoch 137/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1551 - rmse: 0.2176 - r_square: 0.9992 - val_loss: 0.1662 - val_rmse: 0.2361 - val_r_square: 0.9992\n",
      "Epoch 138/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1566 - rmse: 0.2176 - r_square: 0.9992 - val_loss: 0.3576 - val_rmse: 0.2817 - val_r_square: 0.9982\n",
      "Epoch 139/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1547 - rmse: 0.2164 - r_square: 0.9992 - val_loss: 0.1208 - val_rmse: 0.2217 - val_r_square: 0.9994\n",
      "Epoch 140/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1507 - rmse: 0.2144 - r_square: 0.9992 - val_loss: 0.1376 - val_rmse: 0.2107 - val_r_square: 0.9993\n",
      "Epoch 141/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1512 - rmse: 0.2142 - r_square: 0.9992 - val_loss: 0.5495 - val_rmse: 0.4581 - val_r_square: 0.9972\n",
      "Epoch 142/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1515 - rmse: 0.2150 - r_square: 0.9992 - val_loss: 0.5776 - val_rmse: 0.3800 - val_r_square: 0.9971\n",
      "Epoch 143/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1572 - rmse: 0.2170 - r_square: 0.9992 - val_loss: 0.1741 - val_rmse: 0.2217 - val_r_square: 0.9991\n",
      "Epoch 144/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1486 - rmse: 0.2139 - r_square: 0.9992 - val_loss: 0.1062 - val_rmse: 0.1880 - val_r_square: 0.9995\n",
      "Epoch 145/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1545 - rmse: 0.2159 - r_square: 0.9992 - val_loss: 0.1271 - val_rmse: 0.2041 - val_r_square: 0.9994\n",
      "Epoch 146/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1500 - rmse: 0.2133 - r_square: 0.9992 - val_loss: 0.1317 - val_rmse: 0.2171 - val_r_square: 0.9993\n",
      "Epoch 147/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1554 - rmse: 0.2164 - r_square: 0.9992 - val_loss: 0.1059 - val_rmse: 0.1910 - val_r_square: 0.9995\n",
      "Epoch 148/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1462 - rmse: 0.2122 - r_square: 0.9993 - val_loss: 0.1348 - val_rmse: 0.2001 - val_r_square: 0.9993\n",
      "Epoch 149/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1494 - rmse: 0.2133 - r_square: 0.9992 - val_loss: 0.1325 - val_rmse: 0.2073 - val_r_square: 0.9993\n",
      "Epoch 150/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1499 - rmse: 0.2126 - r_square: 0.9992 - val_loss: 0.1636 - val_rmse: 0.2173 - val_r_square: 0.9992\n",
      "Epoch 151/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1451 - rmse: 0.2112 - r_square: 0.9993 - val_loss: 0.0943 - val_rmse: 0.1761 - val_r_square: 0.9995\n",
      "Epoch 152/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1526 - rmse: 0.2138 - r_square: 0.9992 - val_loss: 0.1148 - val_rmse: 0.1949 - val_r_square: 0.9994\n",
      "Epoch 153/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1505 - rmse: 0.2135 - r_square: 0.9992 - val_loss: 0.1187 - val_rmse: 0.1936 - val_r_square: 0.9994\n",
      "Epoch 154/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1547 - rmse: 0.2146 - r_square: 0.9992 - val_loss: 0.1092 - val_rmse: 0.1859 - val_r_square: 0.9994\n",
      "Epoch 155/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1527 - rmse: 0.2148 - r_square: 0.9992 - val_loss: 0.1212 - val_rmse: 0.1967 - val_r_square: 0.9994\n",
      "Epoch 156/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1506 - rmse: 0.2135 - r_square: 0.9992 - val_loss: 0.1510 - val_rmse: 0.2162 - val_r_square: 0.9992\n",
      "Epoch 157/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1488 - rmse: 0.2114 - r_square: 0.9992 - val_loss: 0.1771 - val_rmse: 0.2320 - val_r_square: 0.9991\n",
      "Epoch 158/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1413 - rmse: 0.2088 - r_square: 0.9993 - val_loss: 0.1552 - val_rmse: 0.2048 - val_r_square: 0.9992\n",
      "Epoch 159/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1424 - rmse: 0.2095 - r_square: 0.9993 - val_loss: 0.1188 - val_rmse: 0.1985 - val_r_square: 0.9994\n",
      "Epoch 160/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1431 - rmse: 0.2091 - r_square: 0.9993 - val_loss: 0.1493 - val_rmse: 0.2214 - val_r_square: 0.9992\n",
      "Epoch 161/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1450 - rmse: 0.2104 - r_square: 0.9993 - val_loss: 0.4464 - val_rmse: 0.3433 - val_r_square: 0.9978\n",
      "Epoch 162/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1463 - rmse: 0.2110 - r_square: 0.9992 - val_loss: 0.2403 - val_rmse: 0.2577 - val_r_square: 0.9988\n",
      "Epoch 163/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1476 - rmse: 0.2106 - r_square: 0.9992 - val_loss: 0.5613 - val_rmse: 0.3822 - val_r_square: 0.9971\n",
      "Epoch 164/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1457 - rmse: 0.2099 - r_square: 0.9993 - val_loss: 0.1091 - val_rmse: 0.1852 - val_r_square: 0.9994\n",
      "Epoch 165/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1483 - rmse: 0.2119 - r_square: 0.9992 - val_loss: 0.2128 - val_rmse: 0.2366 - val_r_square: 0.9989\n",
      "Epoch 166/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1470 - rmse: 0.2102 - r_square: 0.9992 - val_loss: 0.1055 - val_rmse: 0.1817 - val_r_square: 0.9995\n",
      "Epoch 167/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1449 - rmse: 0.2096 - r_square: 0.9993 - val_loss: 0.1961 - val_rmse: 0.2367 - val_r_square: 0.9990\n",
      "Epoch 168/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1450 - rmse: 0.2093 - r_square: 0.9993 - val_loss: 0.2187 - val_rmse: 0.2557 - val_r_square: 0.9989\n",
      "Epoch 169/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1400 - rmse: 0.2073 - r_square: 0.9993 - val_loss: 0.1046 - val_rmse: 0.1839 - val_r_square: 0.9995\n",
      "Epoch 170/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1454 - rmse: 0.2094 - r_square: 0.9993 - val_loss: 0.2505 - val_rmse: 0.2670 - val_r_square: 0.9987\n",
      "Epoch 171/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1372 - rmse: 0.2055 - r_square: 0.9993 - val_loss: 0.1712 - val_rmse: 0.2096 - val_r_square: 0.9991\n",
      "Epoch 172/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1446 - rmse: 0.2096 - r_square: 0.9993 - val_loss: 0.1059 - val_rmse: 0.1825 - val_r_square: 0.9995\n",
      "Epoch 173/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1383 - rmse: 0.2059 - r_square: 0.9993 - val_loss: 0.1014 - val_rmse: 0.1772 - val_r_square: 0.9995\n",
      "Epoch 174/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1442 - rmse: 0.2077 - r_square: 0.9993 - val_loss: 0.1497 - val_rmse: 0.2141 - val_r_square: 0.9992\n",
      "Epoch 175/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1411 - rmse: 0.2071 - r_square: 0.9993 - val_loss: 0.1221 - val_rmse: 0.1928 - val_r_square: 0.9994\n",
      "Epoch 176/200\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.1452 - rmse: 0.2078 - r_square: 0.9993 - val_loss: 0.1001 - val_rmse: 0.1776 - val_r_square: 0.9995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00176: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "result = model.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 200,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.38375092e-03]\n",
      " [1.10165656e-01]\n",
      " [1.05316198e+00]\n",
      " [1.69939148e+00]\n",
      " [6.68601990e-02]\n",
      " [1.29932280e+01]\n",
      " [2.95857353e+01]\n",
      " [8.71411800e+00]\n",
      " [9.62109375e+00]\n",
      " [4.82769356e+01]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.000e-02, 1.200e-01, 1.120e+00, 1.500e+00, 4.000e-02, 1.255e+01,\n",
       "       3.055e+01, 8.300e+00, 9.400e+00, 5.065e+01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curves including R^2 and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training curve for rmse\n",
    "plt.plot(result.history['rmse'])\n",
    "plt.plot(result.history['val_rmse'])\n",
    "plt.title('rmse')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training curve for R^2\n",
    "plt.plot(result.history['r_square'])\n",
    "plt.plot(result.history['val_r_square'])\n",
    "plt.title('model R^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.177054\n",
      "Mean squared error (MSE):       0.099708\n",
      "Root mean squared error (RMSE): 0.315766\n",
      "R square (R^2):                 0.999515\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as skm, math\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 21:27:21.248737 10472 deprecation.py:506] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1247590 samples, validate on 311898 samples\n",
      "Epoch 1/200\n",
      "1247590/1247590 [==============================] - 8s 6us/step - loss: 65.3871 - rmse: 4.0208 - r_square: 0.6751 - val_loss: 5.0371 - val_rmse: 1.1821 - val_r_square: 0.9752\n",
      "Epoch 2/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 8.0364 - rmse: 1.4672 - r_square: 0.9596 - val_loss: 1.8562 - val_rmse: 0.7391 - val_r_square: 0.9907\n",
      "Epoch 3/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6307 - rmse: 1.1076 - r_square: 0.9767 - val_loss: 2.0075 - val_rmse: 0.7295 - val_r_square: 0.9899\n",
      "Epoch 4/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.7413 - rmse: 1.0000 - r_square: 0.9812 - val_loss: 2.2993 - val_rmse: 0.8243 - val_r_square: 0.9887\n",
      "Epoch 5/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.1775 - rmse: 0.9317 - r_square: 0.9839 - val_loss: 1.7080 - val_rmse: 0.7222 - val_r_square: 0.9915\n",
      "Epoch 6/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.6620 - rmse: 0.8649 - r_square: 0.9865 - val_loss: 3.5201 - val_rmse: 0.9114 - val_r_square: 0.9827\n",
      "Epoch 7/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.4073 - rmse: 0.8244 - r_square: 0.9879 - val_loss: 1.1668 - val_rmse: 0.5997 - val_r_square: 0.9943\n",
      "Epoch 8/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.2319 - rmse: 0.7962 - r_square: 0.9887 - val_loss: 1.6875 - val_rmse: 0.7200 - val_r_square: 0.9916\n",
      "Epoch 9/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.0116 - rmse: 0.7595 - r_square: 0.9898 - val_loss: 1.5055 - val_rmse: 0.7773 - val_r_square: 0.9925\n",
      "Epoch 10/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.9395 - rmse: 0.7493 - r_square: 0.9901 - val_loss: 2.2151 - val_rmse: 0.8116 - val_r_square: 0.9891\n",
      "Epoch 11/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.8637 - rmse: 0.7264 - r_square: 0.9906 - val_loss: 1.4954 - val_rmse: 0.6673 - val_r_square: 0.9926\n",
      "Epoch 12/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.7006 - rmse: 0.6979 - r_square: 0.9914 - val_loss: 2.2321 - val_rmse: 0.8107 - val_r_square: 0.9889\n",
      "Epoch 13/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.7802 - rmse: 0.7071 - r_square: 0.9910 - val_loss: 1.0830 - val_rmse: 0.6038 - val_r_square: 0.9946\n",
      "Epoch 14/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.5207 - rmse: 0.6661 - r_square: 0.9923 - val_loss: 1.7839 - val_rmse: 0.8282 - val_r_square: 0.9911\n",
      "Epoch 15/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.5485 - rmse: 0.6636 - r_square: 0.9921 - val_loss: 0.9603 - val_rmse: 0.5620 - val_r_square: 0.9952\n",
      "Epoch 16/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.4503 - rmse: 0.6481 - r_square: 0.9926 - val_loss: 1.4970 - val_rmse: 0.6827 - val_r_square: 0.9926\n",
      "Epoch 17/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.3981 - rmse: 0.6408 - r_square: 0.9929 - val_loss: 1.3302 - val_rmse: 0.5954 - val_r_square: 0.9934\n",
      "Epoch 18/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.3142 - rmse: 0.6202 - r_square: 0.9933 - val_loss: 2.1117 - val_rmse: 0.7489 - val_r_square: 0.9896\n",
      "Epoch 19/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.3462 - rmse: 0.6249 - r_square: 0.9932 - val_loss: 1.0594 - val_rmse: 0.6878 - val_r_square: 0.9946\n",
      "Epoch 20/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.2949 - rmse: 0.6133 - r_square: 0.9934 - val_loss: 1.7551 - val_rmse: 0.7587 - val_r_square: 0.9912\n",
      "Epoch 21/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.2428 - rmse: 0.6058 - r_square: 0.9937 - val_loss: 2.5454 - val_rmse: 0.8461 - val_r_square: 0.9874\n",
      "Epoch 22/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.1920 - rmse: 0.5917 - r_square: 0.9940 - val_loss: 1.7318 - val_rmse: 0.7732 - val_r_square: 0.9913\n",
      "Epoch 23/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.1681 - rmse: 0.5860 - r_square: 0.9941 - val_loss: 0.9314 - val_rmse: 0.5538 - val_r_square: 0.9953\n",
      "Epoch 24/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.1990 - rmse: 0.5888 - r_square: 0.9939 - val_loss: 2.3784 - val_rmse: 0.8318 - val_r_square: 0.9882\n",
      "Epoch 25/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.1761 - rmse: 0.5810 - r_square: 0.9940 - val_loss: 1.6175 - val_rmse: 0.6701 - val_r_square: 0.9920\n",
      "Epoch 26/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.0611 - rmse: 0.5640 - r_square: 0.9946 - val_loss: 1.3492 - val_rmse: 0.7123 - val_r_square: 0.9933\n",
      "Epoch 27/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.9898 - rmse: 0.5505 - r_square: 0.9950 - val_loss: 2.3420 - val_rmse: 0.8750 - val_r_square: 0.9883\n",
      "Epoch 28/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.0767 - rmse: 0.5624 - r_square: 0.9945 - val_loss: 1.2942 - val_rmse: 0.5874 - val_r_square: 0.9936\n",
      "Epoch 29/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.0567 - rmse: 0.5541 - r_square: 0.9946 - val_loss: 0.9472 - val_rmse: 0.5327 - val_r_square: 0.9953\n",
      "Epoch 30/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.9446 - rmse: 0.5372 - r_square: 0.9952 - val_loss: 1.6948 - val_rmse: 0.6766 - val_r_square: 0.9916\n",
      "Epoch 31/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.9535 - rmse: 0.5360 - r_square: 0.9951 - val_loss: 1.2514 - val_rmse: 0.6446 - val_r_square: 0.9937\n",
      "Epoch 32/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.0406 - rmse: 0.5478 - r_square: 0.9947 - val_loss: 1.2004 - val_rmse: 0.6020 - val_r_square: 0.9940\n",
      "Epoch 33/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.9368 - rmse: 0.5334 - r_square: 0.9952 - val_loss: 1.6927 - val_rmse: 0.7282 - val_r_square: 0.9916\n",
      "Epoch 34/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.9445 - rmse: 0.5327 - r_square: 0.9952 - val_loss: 1.4893 - val_rmse: 0.6465 - val_r_square: 0.9927\n",
      "Epoch 35/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.9201 - rmse: 0.5250 - r_square: 0.9953 - val_loss: 1.3301 - val_rmse: 0.6298 - val_r_square: 0.9934\n",
      "Epoch 36/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.8981 - rmse: 0.5210 - r_square: 0.9954 - val_loss: 1.1715 - val_rmse: 0.6331 - val_r_square: 0.9941\n",
      "Epoch 37/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.8644 - rmse: 0.5163 - r_square: 0.9956 - val_loss: 1.0426 - val_rmse: 0.6304 - val_r_square: 0.9947\n",
      "Epoch 38/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.8686 - rmse: 0.5139 - r_square: 0.9956 - val_loss: 1.6513 - val_rmse: 0.6849 - val_r_square: 0.9918\n",
      "Epoch 39/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.8815 - rmse: 0.5126 - r_square: 0.9955 - val_loss: 1.4307 - val_rmse: 0.6796 - val_r_square: 0.9929\n",
      "Epoch 40/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.7896 - rmse: 0.4966 - r_square: 0.9960 - val_loss: 1.6669 - val_rmse: 0.6969 - val_r_square: 0.9918\n",
      "Epoch 41/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.7817 - rmse: 0.4938 - r_square: 0.9960 - val_loss: 1.1564 - val_rmse: 0.6029 - val_r_square: 0.9942\n",
      "Epoch 42/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.8392 - rmse: 0.5005 - r_square: 0.9957 - val_loss: 1.7365 - val_rmse: 0.7323 - val_r_square: 0.9913\n",
      "Epoch 43/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.7898 - rmse: 0.4927 - r_square: 0.9960 - val_loss: 1.1633 - val_rmse: 0.6390 - val_r_square: 0.9942\n",
      "Epoch 44/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.7667 - rmse: 0.4863 - r_square: 0.9961 - val_loss: 1.1365 - val_rmse: 0.7045 - val_r_square: 0.9942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.7662 - rmse: 0.4876 - r_square: 0.9961 - val_loss: 1.1892 - val_rmse: 0.6234 - val_r_square: 0.9940\n",
      "Epoch 46/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.7385 - rmse: 0.4804 - r_square: 0.9962 - val_loss: 0.9328 - val_rmse: 0.5534 - val_r_square: 0.9953\n",
      "Epoch 47/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.7387 - rmse: 0.4775 - r_square: 0.9962 - val_loss: 1.3995 - val_rmse: 0.6575 - val_r_square: 0.9930\n",
      "Epoch 48/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.7239 - rmse: 0.4755 - r_square: 0.9963 - val_loss: 1.0100 - val_rmse: 0.6035 - val_r_square: 0.9949\n",
      "Epoch 00048: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dropout(0.1),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "model2.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")\n",
    "# fit the model\n",
    "result2 = model2.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 200,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.9846809e-02]\n",
      " [9.5317960e-02]\n",
      " [6.8059593e-01]\n",
      " [2.5958467e+00]\n",
      " [4.4982195e-02]\n",
      " [1.2285782e+01]\n",
      " [2.7649044e+01]\n",
      " [8.6950426e+00]\n",
      " [1.0728182e+01]\n",
      " [4.8144070e+01]]\n",
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.602090\n",
      "Mean squared error (MSE):       1.004913\n",
      "Root mean squared error (RMSE): 1.002453\n",
      "R square (R^2):                 0.995114\n"
     ]
    }
   ],
   "source": [
    "predictions2 = model2.predict(X_test)\n",
    "print(predictions2[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions2))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions2))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions2)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try batch normalization technique to regularize neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1247590 samples, validate on 311898 samples\n",
      "Epoch 1/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 20.2347 - rmse: 1.9044 - r_square: 0.8994 - val_loss: 2.9283 - val_rmse: 0.7770 - val_r_square: 0.9853\n",
      "Epoch 2/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 6.8119 - rmse: 1.2502 - r_square: 0.9656 - val_loss: 5.2465 - val_rmse: 1.0042 - val_r_square: 0.9737\n",
      "Epoch 3/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 6.2657 - rmse: 1.1880 - r_square: 0.9682 - val_loss: 2.1644 - val_rmse: 0.7949 - val_r_square: 0.9890\n",
      "Epoch 4/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.8657 - rmse: 1.1516 - r_square: 0.9703 - val_loss: 2.3394 - val_rmse: 0.7156 - val_r_square: 0.9884\n",
      "Epoch 5/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.7647 - rmse: 1.1419 - r_square: 0.9710 - val_loss: 1.8557 - val_rmse: 0.7330 - val_r_square: 0.9905\n",
      "Epoch 6/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.6440 - rmse: 1.1162 - r_square: 0.9714 - val_loss: 1.5048 - val_rmse: 0.5708 - val_r_square: 0.9924\n",
      "Epoch 7/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.3207 - rmse: 1.0896 - r_square: 0.9731 - val_loss: 1.8653 - val_rmse: 0.7059 - val_r_square: 0.9906\n",
      "Epoch 8/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.4349 - rmse: 1.0856 - r_square: 0.9726 - val_loss: 1.4729 - val_rmse: 0.6323 - val_r_square: 0.9925\n",
      "Epoch 9/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.2597 - rmse: 1.0674 - r_square: 0.9736 - val_loss: 1.2567 - val_rmse: 0.5660 - val_r_square: 0.9937\n",
      "Epoch 10/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.2848 - rmse: 1.0673 - r_square: 0.9734 - val_loss: 1.2359 - val_rmse: 0.5377 - val_r_square: 0.9937\n",
      "Epoch 11/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.2453 - rmse: 1.0649 - r_square: 0.9736 - val_loss: 1.2803 - val_rmse: 0.5903 - val_r_square: 0.9935\n",
      "Epoch 12/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.1947 - rmse: 1.0480 - r_square: 0.9739 - val_loss: 2.2452 - val_rmse: 0.7276 - val_r_square: 0.9888\n",
      "Epoch 13/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.0747 - rmse: 1.0442 - r_square: 0.9745 - val_loss: 2.0478 - val_rmse: 0.6492 - val_r_square: 0.9896\n",
      "Epoch 14/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.0801 - rmse: 1.0409 - r_square: 0.9743 - val_loss: 2.7386 - val_rmse: 0.7747 - val_r_square: 0.9865\n",
      "Epoch 15/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.0273 - rmse: 1.0318 - r_square: 0.9746 - val_loss: 1.0484 - val_rmse: 0.5042 - val_r_square: 0.9948\n",
      "Epoch 16/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.9575 - rmse: 1.0264 - r_square: 0.9749 - val_loss: 1.1245 - val_rmse: 0.5210 - val_r_square: 0.9943\n",
      "Epoch 17/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.9781 - rmse: 1.0225 - r_square: 0.9748 - val_loss: 1.4062 - val_rmse: 0.6760 - val_r_square: 0.9929\n",
      "Epoch 18/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.9295 - rmse: 1.0221 - r_square: 0.9750 - val_loss: 1.0384 - val_rmse: 0.5225 - val_r_square: 0.9948\n",
      "Epoch 19/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.9476 - rmse: 1.0215 - r_square: 0.9750 - val_loss: 1.2243 - val_rmse: 0.5196 - val_r_square: 0.9938\n",
      "Epoch 20/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 5.0000 - rmse: 1.0209 - r_square: 0.9747 - val_loss: 1.0935 - val_rmse: 0.5362 - val_r_square: 0.9945\n",
      "Epoch 21/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.9661 - rmse: 1.0244 - r_square: 0.9749 - val_loss: 0.8766 - val_rmse: 0.4786 - val_r_square: 0.9956\n",
      "Epoch 22/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.8685 - rmse: 1.0082 - r_square: 0.9754 - val_loss: 1.2698 - val_rmse: 0.6043 - val_r_square: 0.9936\n",
      "Epoch 23/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.8253 - rmse: 1.0132 - r_square: 0.9756 - val_loss: 1.3010 - val_rmse: 0.5470 - val_r_square: 0.9935\n",
      "Epoch 24/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6994 - rmse: 0.9997 - r_square: 0.9761 - val_loss: 1.0424 - val_rmse: 0.5136 - val_r_square: 0.9948\n",
      "Epoch 25/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.7967 - rmse: 1.0040 - r_square: 0.9757 - val_loss: 1.0160 - val_rmse: 0.5196 - val_r_square: 0.9949\n",
      "Epoch 26/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.8295 - rmse: 1.0048 - r_square: 0.9755 - val_loss: 1.1493 - val_rmse: 0.4842 - val_r_square: 0.9943\n",
      "Epoch 27/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.7007 - rmse: 0.9907 - r_square: 0.9762 - val_loss: 1.5112 - val_rmse: 0.5850 - val_r_square: 0.9924\n",
      "Epoch 28/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6802 - rmse: 0.9947 - r_square: 0.9764 - val_loss: 1.1853 - val_rmse: 0.5547 - val_r_square: 0.9940\n",
      "Epoch 29/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.8782 - rmse: 1.0066 - r_square: 0.9754 - val_loss: 1.1539 - val_rmse: 0.5191 - val_r_square: 0.9942\n",
      "Epoch 30/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6547 - rmse: 0.9892 - r_square: 0.9765 - val_loss: 2.4044 - val_rmse: 0.7563 - val_r_square: 0.9878\n",
      "Epoch 31/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.7114 - rmse: 0.9904 - r_square: 0.9762 - val_loss: 1.0295 - val_rmse: 0.4783 - val_r_square: 0.9949\n",
      "Epoch 32/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6500 - rmse: 0.9865 - r_square: 0.9765 - val_loss: 0.9803 - val_rmse: 0.4617 - val_r_square: 0.9951\n",
      "Epoch 33/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6062 - rmse: 0.9783 - r_square: 0.9768 - val_loss: 1.2283 - val_rmse: 0.5076 - val_r_square: 0.9938\n",
      "Epoch 34/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.7156 - rmse: 0.9861 - r_square: 0.9762 - val_loss: 1.6368 - val_rmse: 0.5729 - val_r_square: 0.9917\n",
      "Epoch 35/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6823 - rmse: 0.9860 - r_square: 0.9762 - val_loss: 2.5502 - val_rmse: 0.7412 - val_r_square: 0.9870\n",
      "Epoch 36/200\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 4.7259 - rmse: 0.9937 - r_square: 0.9761 - val_loss: 1.1772 - val_rmse: 0.4889 - val_r_square: 0.9941\n",
      "Epoch 37/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.7061 - rmse: 0.9825 - r_square: 0.9762 - val_loss: 1.2511 - val_rmse: 0.5685 - val_r_square: 0.9937\n",
      "Epoch 38/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6948 - rmse: 0.9921 - r_square: 0.9762 - val_loss: 1.2016 - val_rmse: 0.4894 - val_r_square: 0.9939\n",
      "Epoch 39/200\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 4.7159 - rmse: 0.9842 - r_square: 0.9763 - val_loss: 1.1355 - val_rmse: 0.4543 - val_r_square: 0.9943\n",
      "Epoch 40/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6087 - rmse: 0.9827 - r_square: 0.9769 - val_loss: 1.3271 - val_rmse: 0.5506 - val_r_square: 0.9933\n",
      "Epoch 41/200\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 4.6285 - rmse: 0.9820 - r_square: 0.9766 - val_loss: 1.4851 - val_rmse: 0.5772 - val_r_square: 0.9925\n",
      "Epoch 42/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.7220 - rmse: 0.9858 - r_square: 0.9760 - val_loss: 0.9797 - val_rmse: 0.4528 - val_r_square: 0.9951\n",
      "Epoch 43/200\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 4.5822 - rmse: 0.9728 - r_square: 0.9767 - val_loss: 1.0976 - val_rmse: 0.5016 - val_r_square: 0.9945\n",
      "Epoch 44/200\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 4.6356 - rmse: 0.9780 - r_square: 0.9767 - val_loss: 1.4294 - val_rmse: 0.5874 - val_r_square: 0.9928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 4.6773 - rmse: 0.9831 - r_square: 0.9763 - val_loss: 1.1762 - val_rmse: 0.5392 - val_r_square: 0.9941\n",
      "Epoch 46/200\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 4.6474 - rmse: 0.9798 - r_square: 0.9766 - val_loss: 1.0842 - val_rmse: 0.4909 - val_r_square: 0.9945\n",
      "Epoch 00046: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "model3 = Sequential([\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "model3.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")\n",
    "# fit the model\n",
    "result3 = model3.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 200,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05163175]\n",
      " [ 0.11092597]\n",
      " [ 1.0757746 ]\n",
      " [ 1.625279  ]\n",
      " [ 0.13878578]\n",
      " [13.080181  ]\n",
      " [32.02581   ]\n",
      " [ 9.127215  ]\n",
      " [10.870003  ]\n",
      " [46.180584  ]]\n",
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.490583\n",
      "Mean squared error (MSE):       1.087603\n",
      "Root mean squared error (RMSE): 1.042882\n",
      "R square (R^2):                 0.994712\n"
     ]
    }
   ],
   "source": [
    "predictions3 = model3.predict(X_test)\n",
    "print(predictions3[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions3))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions3))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions3)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: \n",
    "&emsp;&emsp;What does 'Accuracy' mean in Regression? https://github.com/keras-team/keras/issues/7947 <br>\n",
    "&emsp;&emsp;https://keras.io/metrics/<br>\n",
    "&emsp;&emsp;Scale, Standardize, or Normalize with Scikit-Learn https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02 <br>\n",
    "&emsp;&emsp;The Day my Computer Won the Nobel Prize (Neural Network Option Pricing)  https://medium.com/datadriveninvestor/the-day-my-computer-won-the-nobel-prize-neural-network-option-pricing-d29b4379f1d2 <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
