{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-19.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Requirement already satisfied: pyyaml in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: h5py in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.14.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.11.0)\n",
      "Installing collected packages: keras-applications, keras-preprocessing, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/4a/5c86ed8b245aa48f9f819b13a0a9039e9126ba19fdd0c7e0b8026c12315a/tensorflow-1.14.0-cp36-cp36m-win_amd64.whl (68.3MB)\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/ed/e036d31a9b2c750f270cbb1cfc1c0f94ac78ae504eea7eec3267be4e294a/numpy-1.16.4-cp36-cp36m-win_amd64.whl (11.9MB)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.1)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/59/ec6c6075dd628f2efd33dbe11b259e63e1de7cbc26e45b38de767df528d4/protobuf-3.8.0-cp36-cp36m-win_amd64.whl (1.1MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.1.0)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/78/93/297ff3656f1073fba84e2f9633ad3b27a007eb59ad22099ac30142f80365/grpcio-1.22.0-cp36-cp36m-win_amd64.whl (1.6MB)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.0.8)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Requirement already satisfied: setuptools in c:\\program files\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: h5py in c:\\program files\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.1)\n",
      "Building wheels for collected packages: gast, absl-py, termcolor, wrapt\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\ee\\98\\38\\46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\d7\\de\\2e\\efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "Successfully built gast absl-py termcolor wrapt\n",
      "Installing collected packages: numpy, google-pasta, gast, absl-py, tensorflow-estimator, protobuf, grpcio, markdown, tensorboard, astor, termcolor, wrapt, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.22.0 markdown-3.1.1 numpy-1.16.4 protobuf-3.8.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 wrapt-1.11.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorboard 1.14.0 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts freeze_graph.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user keras\n",
    "!pip install --user tensorflow\n",
    "!pip install --user -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU only\n",
    "# Only Macbook needs to run this cell\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and do data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>exdate</th>\n",
       "      <th>maturity</th>\n",
       "      <th>strike_price</th>\n",
       "      <th>best_offer</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>underlying_price</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>cp_flag_C</th>\n",
       "      <th>cp_flag_P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.181376</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>62.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.450289</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>67.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.369425</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.888123</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date      exdate  maturity  strike_price  best_offer  \\\n",
       "0    AXP  2013-01-02  2013-01-04         2          60.0        0.03   \n",
       "1    AXP  2013-01-02  2013-01-04         2          62.5        0.05   \n",
       "2    AXP  2013-01-02  2013-01-04         2          65.0        0.05   \n",
       "3    AXP  2013-01-02  2013-01-04         2          67.5        0.50   \n",
       "4    AXP  2013-01-02  2013-01-04         2          70.0        0.01   \n",
       "\n",
       "   impl_volatility  underlying_price  interest_rate  cp_flag_C  cp_flag_P  \n",
       "0         0.181376             58.75         0.0008          1          0  \n",
       "1         0.450289             58.75         0.0008          1          0  \n",
       "2         0.676564             58.75         0.0008          1          0  \n",
       "3         1.369425             58.75         0.0008          1          0  \n",
       "4         0.888123             58.75         0.0008          1          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Options.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['interest_rate'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['best_offer']\n",
    "X = df[['maturity', 'strike_price', 'underlying_price', 'cp_flag_C', 'cp_flag_P', 'interest_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to build a regression neural network model\n",
    "sc= preprocessing.MinMaxScaler()\n",
    "X= sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1559488, 6)\n",
      "(1559488,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train, XX_validation, yy_train, yy_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a keras Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 20:23:12.838638 10036 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0705 20:23:12.853649 10036 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0705 20:23:12.858652 10036 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Built Keras sequential model with 3 hidden layer, and after the first hidden layer.\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable early stopping based on the loss of validation data\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because it is a simple regression problem, we should custom metrics function\n",
    "from keras import backend as K\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis = -1))\n",
    "\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 20:23:23.027426 10036 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use mean_squared_error to compile regression model loss\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 20:23:43.079288 10036 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0705 20:23:43.224327 10036 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1247590 samples, validate on 311898 samples\n",
      "Epoch 1/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 9.4229 - rmse: 0.8669 - r_square: 0.9537 - val_loss: 0.4624 - val_rmse: 0.4395 - val_r_square: 0.9976\n",
      "Epoch 2/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3911 - rmse: 0.3857 - r_square: 0.9980 - val_loss: 0.3830 - val_rmse: 0.3720 - val_r_square: 0.9980\n",
      "Epoch 3/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3707 - rmse: 0.3761 - r_square: 0.9981 - val_loss: 0.3912 - val_rmse: 0.3945 - val_r_square: 0.9980\n",
      "Epoch 4/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3421 - rmse: 0.3625 - r_square: 0.9982 - val_loss: 0.2950 - val_rmse: 0.3327 - val_r_square: 0.9985\n",
      "Epoch 5/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3222 - rmse: 0.3522 - r_square: 0.9983 - val_loss: 0.3065 - val_rmse: 0.3346 - val_r_square: 0.9984\n",
      "Epoch 6/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3095 - rmse: 0.3445 - r_square: 0.9984 - val_loss: 0.3459 - val_rmse: 0.3855 - val_r_square: 0.9982\n",
      "Epoch 7/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.3009 - rmse: 0.3394 - r_square: 0.9984 - val_loss: 0.3612 - val_rmse: 0.4161 - val_r_square: 0.9981\n",
      "Epoch 8/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2975 - rmse: 0.3369 - r_square: 0.9985 - val_loss: 0.2673 - val_rmse: 0.3140 - val_r_square: 0.9986\n",
      "Epoch 9/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2911 - rmse: 0.3325 - r_square: 0.9985 - val_loss: 0.2552 - val_rmse: 0.3028 - val_r_square: 0.9987\n",
      "Epoch 10/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2878 - rmse: 0.3303 - r_square: 0.9985 - val_loss: 0.2642 - val_rmse: 0.3170 - val_r_square: 0.9986\n",
      "Epoch 11/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2858 - rmse: 0.3288 - r_square: 0.9985 - val_loss: 0.2993 - val_rmse: 0.3398 - val_r_square: 0.9985\n",
      "Epoch 12/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2831 - rmse: 0.3272 - r_square: 0.9985 - val_loss: 0.3027 - val_rmse: 0.3228 - val_r_square: 0.9984\n",
      "Epoch 13/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2802 - rmse: 0.3254 - r_square: 0.9985 - val_loss: 0.3043 - val_rmse: 0.3541 - val_r_square: 0.9984\n",
      "Epoch 14/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2783 - rmse: 0.3246 - r_square: 0.9986 - val_loss: 0.3029 - val_rmse: 0.3291 - val_r_square: 0.9984\n",
      "Epoch 15/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2738 - rmse: 0.3214 - r_square: 0.9986 - val_loss: 0.2709 - val_rmse: 0.3129 - val_r_square: 0.9986\n",
      "Epoch 16/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2727 - rmse: 0.3208 - r_square: 0.9986 - val_loss: 0.2543 - val_rmse: 0.3069 - val_r_square: 0.9987\n",
      "Epoch 17/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2700 - rmse: 0.3195 - r_square: 0.9986 - val_loss: 0.2480 - val_rmse: 0.3050 - val_r_square: 0.9987\n",
      "Epoch 18/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2673 - rmse: 0.3176 - r_square: 0.9986 - val_loss: 0.3012 - val_rmse: 0.3281 - val_r_square: 0.9984\n",
      "Epoch 19/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2646 - rmse: 0.3159 - r_square: 0.9986 - val_loss: 0.2567 - val_rmse: 0.3116 - val_r_square: 0.9987\n",
      "Epoch 20/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2635 - rmse: 0.3154 - r_square: 0.9986 - val_loss: 0.3033 - val_rmse: 0.3348 - val_r_square: 0.9984\n",
      "Epoch 21/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2628 - rmse: 0.3148 - r_square: 0.9986 - val_loss: 0.2607 - val_rmse: 0.3117 - val_r_square: 0.9987\n",
      "Epoch 22/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2598 - rmse: 0.3128 - r_square: 0.9986 - val_loss: 0.2447 - val_rmse: 0.3023 - val_r_square: 0.9987\n",
      "Epoch 23/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2594 - rmse: 0.3124 - r_square: 0.9987 - val_loss: 0.2664 - val_rmse: 0.3264 - val_r_square: 0.9986\n",
      "Epoch 24/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2572 - rmse: 0.3107 - r_square: 0.9987 - val_loss: 0.2793 - val_rmse: 0.3315 - val_r_square: 0.9986\n",
      "Epoch 25/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2561 - rmse: 0.3102 - r_square: 0.9987 - val_loss: 0.2523 - val_rmse: 0.2965 - val_r_square: 0.9987\n",
      "Epoch 26/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2540 - rmse: 0.3087 - r_square: 0.9987 - val_loss: 0.2560 - val_rmse: 0.3048 - val_r_square: 0.9987\n",
      "Epoch 27/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2533 - rmse: 0.3086 - r_square: 0.9987 - val_loss: 0.2629 - val_rmse: 0.3210 - val_r_square: 0.9986\n",
      "Epoch 28/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2507 - rmse: 0.3063 - r_square: 0.9987 - val_loss: 0.2369 - val_rmse: 0.2983 - val_r_square: 0.9988\n",
      "Epoch 29/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2497 - rmse: 0.3056 - r_square: 0.9987 - val_loss: 0.2379 - val_rmse: 0.2947 - val_r_square: 0.9988\n",
      "Epoch 30/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2500 - rmse: 0.3057 - r_square: 0.9987 - val_loss: 0.2576 - val_rmse: 0.3084 - val_r_square: 0.9987\n",
      "Epoch 31/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2473 - rmse: 0.3042 - r_square: 0.9987 - val_loss: 0.2252 - val_rmse: 0.2865 - val_r_square: 0.9988\n",
      "Epoch 32/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2460 - rmse: 0.3031 - r_square: 0.9987 - val_loss: 0.2520 - val_rmse: 0.2986 - val_r_square: 0.9987\n",
      "Epoch 33/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2470 - rmse: 0.3040 - r_square: 0.9987 - val_loss: 0.2325 - val_rmse: 0.2961 - val_r_square: 0.9988\n",
      "Epoch 34/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2461 - rmse: 0.3030 - r_square: 0.9987 - val_loss: 0.2348 - val_rmse: 0.2899 - val_r_square: 0.9988\n",
      "Epoch 35/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2433 - rmse: 0.3012 - r_square: 0.9987 - val_loss: 0.2540 - val_rmse: 0.3162 - val_r_square: 0.9987\n",
      "Epoch 36/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2435 - rmse: 0.3013 - r_square: 0.9987 - val_loss: 0.2437 - val_rmse: 0.2954 - val_r_square: 0.9987\n",
      "Epoch 37/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2433 - rmse: 0.3010 - r_square: 0.9987 - val_loss: 0.2424 - val_rmse: 0.2949 - val_r_square: 0.9987\n",
      "Epoch 38/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2415 - rmse: 0.2999 - r_square: 0.9987 - val_loss: 0.2783 - val_rmse: 0.3138 - val_r_square: 0.9986\n",
      "Epoch 39/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2416 - rmse: 0.2996 - r_square: 0.9987 - val_loss: 0.2354 - val_rmse: 0.2978 - val_r_square: 0.9988\n",
      "Epoch 40/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2404 - rmse: 0.2987 - r_square: 0.9988 - val_loss: 0.2401 - val_rmse: 0.3077 - val_r_square: 0.9988\n",
      "Epoch 41/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2409 - rmse: 0.2993 - r_square: 0.9987 - val_loss: 0.2285 - val_rmse: 0.2953 - val_r_square: 0.9988\n",
      "Epoch 42/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2383 - rmse: 0.2973 - r_square: 0.9988 - val_loss: 0.2996 - val_rmse: 0.3345 - val_r_square: 0.9985\n",
      "Epoch 43/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2368 - rmse: 0.2964 - r_square: 0.9988 - val_loss: 0.2246 - val_rmse: 0.2796 - val_r_square: 0.9988\n",
      "Epoch 44/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2360 - rmse: 0.2958 - r_square: 0.9988 - val_loss: 0.2202 - val_rmse: 0.2850 - val_r_square: 0.9989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2353 - rmse: 0.2953 - r_square: 0.9988 - val_loss: 0.2466 - val_rmse: 0.2979 - val_r_square: 0.9987\n",
      "Epoch 46/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2339 - rmse: 0.2946 - r_square: 0.9988 - val_loss: 0.2496 - val_rmse: 0.3010 - val_r_square: 0.9987\n",
      "Epoch 47/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2342 - rmse: 0.2946 - r_square: 0.9988 - val_loss: 0.2249 - val_rmse: 0.2827 - val_r_square: 0.9988\n",
      "Epoch 48/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2315 - rmse: 0.2929 - r_square: 0.9988 - val_loss: 0.2289 - val_rmse: 0.2867 - val_r_square: 0.9988\n",
      "Epoch 49/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2321 - rmse: 0.2935 - r_square: 0.9988 - val_loss: 0.2386 - val_rmse: 0.3001 - val_r_square: 0.9988\n",
      "Epoch 50/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2317 - rmse: 0.2933 - r_square: 0.9988 - val_loss: 0.2303 - val_rmse: 0.2944 - val_r_square: 0.9988\n",
      "Epoch 51/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2312 - rmse: 0.2929 - r_square: 0.9988 - val_loss: 0.2584 - val_rmse: 0.3166 - val_r_square: 0.9987\n",
      "Epoch 52/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2298 - rmse: 0.2922 - r_square: 0.9988 - val_loss: 0.2265 - val_rmse: 0.2966 - val_r_square: 0.9988\n",
      "Epoch 53/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2288 - rmse: 0.2913 - r_square: 0.9988 - val_loss: 0.2460 - val_rmse: 0.3010 - val_r_square: 0.9987\n",
      "Epoch 54/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2281 - rmse: 0.2907 - r_square: 0.9988 - val_loss: 0.2283 - val_rmse: 0.3025 - val_r_square: 0.9988\n",
      "Epoch 55/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2272 - rmse: 0.2904 - r_square: 0.9988 - val_loss: 0.2110 - val_rmse: 0.2795 - val_r_square: 0.9989\n",
      "Epoch 56/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2264 - rmse: 0.2898 - r_square: 0.9988 - val_loss: 0.2566 - val_rmse: 0.3163 - val_r_square: 0.9987\n",
      "Epoch 57/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2259 - rmse: 0.2897 - r_square: 0.9988 - val_loss: 0.2287 - val_rmse: 0.2908 - val_r_square: 0.9988\n",
      "Epoch 58/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2255 - rmse: 0.2896 - r_square: 0.9988 - val_loss: 0.2459 - val_rmse: 0.3004 - val_r_square: 0.9987\n",
      "Epoch 59/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2238 - rmse: 0.2879 - r_square: 0.9988 - val_loss: 0.2227 - val_rmse: 0.2942 - val_r_square: 0.9988\n",
      "Epoch 60/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2235 - rmse: 0.2881 - r_square: 0.9988 - val_loss: 0.2131 - val_rmse: 0.2741 - val_r_square: 0.9989\n",
      "Epoch 61/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2224 - rmse: 0.2873 - r_square: 0.9988 - val_loss: 0.2106 - val_rmse: 0.2796 - val_r_square: 0.9989\n",
      "Epoch 62/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2224 - rmse: 0.2874 - r_square: 0.9988 - val_loss: 0.2538 - val_rmse: 0.3163 - val_r_square: 0.9987\n",
      "Epoch 63/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2214 - rmse: 0.2867 - r_square: 0.9988 - val_loss: 0.2305 - val_rmse: 0.2945 - val_r_square: 0.9988\n",
      "Epoch 64/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2200 - rmse: 0.2857 - r_square: 0.9989 - val_loss: 0.2330 - val_rmse: 0.3019 - val_r_square: 0.9988\n",
      "Epoch 65/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2211 - rmse: 0.2864 - r_square: 0.9989 - val_loss: 0.2332 - val_rmse: 0.2908 - val_r_square: 0.9988\n",
      "Epoch 66/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2208 - rmse: 0.2864 - r_square: 0.9989 - val_loss: 0.2141 - val_rmse: 0.2768 - val_r_square: 0.9989\n",
      "Epoch 67/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2197 - rmse: 0.2856 - r_square: 0.9989 - val_loss: 0.2322 - val_rmse: 0.3022 - val_r_square: 0.9988\n",
      "Epoch 68/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2194 - rmse: 0.2853 - r_square: 0.9989 - val_loss: 0.2331 - val_rmse: 0.3023 - val_r_square: 0.9988\n",
      "Epoch 69/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2183 - rmse: 0.2846 - r_square: 0.9989 - val_loss: 0.2056 - val_rmse: 0.2718 - val_r_square: 0.9989\n",
      "Epoch 70/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2181 - rmse: 0.2844 - r_square: 0.9989 - val_loss: 0.2122 - val_rmse: 0.2799 - val_r_square: 0.9989\n",
      "Epoch 71/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2169 - rmse: 0.2837 - r_square: 0.9989 - val_loss: 0.2231 - val_rmse: 0.2950 - val_r_square: 0.9988\n",
      "Epoch 72/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2170 - rmse: 0.2837 - r_square: 0.9989 - val_loss: 0.2123 - val_rmse: 0.2820 - val_r_square: 0.9989\n",
      "Epoch 73/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2166 - rmse: 0.2836 - r_square: 0.9989 - val_loss: 0.2292 - val_rmse: 0.2985 - val_r_square: 0.9988\n",
      "Epoch 74/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2172 - rmse: 0.2840 - r_square: 0.9989 - val_loss: 0.2156 - val_rmse: 0.2907 - val_r_square: 0.9989\n",
      "Epoch 75/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2158 - rmse: 0.2830 - r_square: 0.9989 - val_loss: 0.2288 - val_rmse: 0.2924 - val_r_square: 0.9988\n",
      "Epoch 76/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2162 - rmse: 0.2832 - r_square: 0.9989 - val_loss: 0.2079 - val_rmse: 0.2767 - val_r_square: 0.9989\n",
      "Epoch 77/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2159 - rmse: 0.2830 - r_square: 0.9989 - val_loss: 0.2110 - val_rmse: 0.2783 - val_r_square: 0.9989\n",
      "Epoch 78/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2145 - rmse: 0.2821 - r_square: 0.9989 - val_loss: 0.2116 - val_rmse: 0.2739 - val_r_square: 0.9989\n",
      "Epoch 79/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2145 - rmse: 0.2820 - r_square: 0.9989 - val_loss: 0.2276 - val_rmse: 0.2889 - val_r_square: 0.9988\n",
      "Epoch 80/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2138 - rmse: 0.2816 - r_square: 0.9989 - val_loss: 0.2118 - val_rmse: 0.2811 - val_r_square: 0.9989\n",
      "Epoch 81/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2135 - rmse: 0.2811 - r_square: 0.9989 - val_loss: 0.2498 - val_rmse: 0.3117 - val_r_square: 0.9987\n",
      "Epoch 82/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2141 - rmse: 0.2819 - r_square: 0.9989 - val_loss: 0.2149 - val_rmse: 0.2807 - val_r_square: 0.9989\n",
      "Epoch 83/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2141 - rmse: 0.2818 - r_square: 0.9989 - val_loss: 0.2053 - val_rmse: 0.2769 - val_r_square: 0.9989\n",
      "Epoch 84/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2126 - rmse: 0.2808 - r_square: 0.9989 - val_loss: 0.2086 - val_rmse: 0.2780 - val_r_square: 0.9989\n",
      "Epoch 85/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2127 - rmse: 0.2808 - r_square: 0.9989 - val_loss: 0.2059 - val_rmse: 0.2761 - val_r_square: 0.9989\n",
      "Epoch 86/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2127 - rmse: 0.2807 - r_square: 0.9989 - val_loss: 0.2291 - val_rmse: 0.2887 - val_r_square: 0.9988\n",
      "Epoch 87/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2113 - rmse: 0.2798 - r_square: 0.9989 - val_loss: 0.2319 - val_rmse: 0.2957 - val_r_square: 0.9988\n",
      "Epoch 88/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2116 - rmse: 0.2801 - r_square: 0.9989 - val_loss: 0.2569 - val_rmse: 0.2960 - val_r_square: 0.9987\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2118 - rmse: 0.2803 - r_square: 0.9989 - val_loss: 0.2270 - val_rmse: 0.2798 - val_r_square: 0.9988\n",
      "Epoch 90/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2113 - rmse: 0.2796 - r_square: 0.9989 - val_loss: 0.2018 - val_rmse: 0.2669 - val_r_square: 0.9990\n",
      "Epoch 91/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2107 - rmse: 0.2794 - r_square: 0.9989 - val_loss: 0.2053 - val_rmse: 0.2714 - val_r_square: 0.9989\n",
      "Epoch 92/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2103 - rmse: 0.2790 - r_square: 0.9989 - val_loss: 0.2350 - val_rmse: 0.2901 - val_r_square: 0.9988\n",
      "Epoch 93/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2102 - rmse: 0.2790 - r_square: 0.9989 - val_loss: 0.2180 - val_rmse: 0.2906 - val_r_square: 0.9989\n",
      "Epoch 94/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2099 - rmse: 0.2790 - r_square: 0.9989 - val_loss: 0.2175 - val_rmse: 0.2960 - val_r_square: 0.9989\n",
      "Epoch 95/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2109 - rmse: 0.2796 - r_square: 0.9989 - val_loss: 0.2255 - val_rmse: 0.2879 - val_r_square: 0.9988\n",
      "Epoch 96/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2091 - rmse: 0.2783 - r_square: 0.9989 - val_loss: 0.2228 - val_rmse: 0.2810 - val_r_square: 0.9988\n",
      "Epoch 97/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2093 - rmse: 0.2783 - r_square: 0.9989 - val_loss: 0.1990 - val_rmse: 0.2712 - val_r_square: 0.9990\n",
      "Epoch 98/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2096 - rmse: 0.2785 - r_square: 0.9989 - val_loss: 0.2030 - val_rmse: 0.2662 - val_r_square: 0.9989\n",
      "Epoch 99/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2084 - rmse: 0.2776 - r_square: 0.9989 - val_loss: 0.2101 - val_rmse: 0.2810 - val_r_square: 0.9989\n",
      "Epoch 100/100\n",
      "1247590/1247590 [==============================] - 6s 5us/step - loss: 0.2083 - rmse: 0.2777 - r_square: 0.9989 - val_loss: 0.2033 - val_rmse: 0.2818 - val_r_square: 0.9989\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "result = model.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 100,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4757307e-02]\n",
      " [8.6236984e-02]\n",
      " [1.0375798e+00]\n",
      " [1.8603920e+00]\n",
      " [1.1812827e-01]\n",
      " [1.3241207e+01]\n",
      " [3.0101727e+01]\n",
      " [8.8977137e+00]\n",
      " [9.4007206e+00]\n",
      " [4.9212929e+01]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593616      0.02\n",
       "1410840     0.12\n",
       "1300486     1.12\n",
       "734479      1.50\n",
       "350525      0.04\n",
       "1653947    12.55\n",
       "1085111    30.55\n",
       "1093566     8.30\n",
       "224536      9.40\n",
       "279697     50.65\n",
       "Name: best_offer, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curves including R^2 and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training curve for rmse\n",
    "plt.plot(result.history['rmse'])\n",
    "plt.plot(result.history['val_rmse'])\n",
    "plt.title('rmse')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training curve for R^2\n",
    "plt.plot(result.history['r_square'])\n",
    "plt.plot(result.history['val_r_square'])\n",
    "plt.title('model R^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.280542\n",
      "Mean squared error (MSE):       0.201587\n",
      "Root mean squared error (RMSE): 0.448984\n",
      "R square (R^2):                 0.999020\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as skm, math\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 20:40:05.340076 10036 deprecation.py:506] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1247590 samples, validate on 311898 samples\n",
      "Epoch 1/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 11.4388 - rmse: 1.3180 - r_square: 0.9420 - val_loss: 0.5858 - val_rmse: 0.4638 - val_r_square: 0.9970\n",
      "Epoch 2/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 1.3110 - rmse: 0.6532 - r_square: 0.9934 - val_loss: 0.9249 - val_rmse: 0.6207 - val_r_square: 0.9953\n",
      "Epoch 3/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.8679 - rmse: 0.5480 - r_square: 0.9956 - val_loss: 1.1892 - val_rmse: 0.6683 - val_r_square: 0.9941\n",
      "Epoch 4/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 0.6364 - rmse: 0.4771 - r_square: 0.9967 - val_loss: 1.4994 - val_rmse: 0.6939 - val_r_square: 0.9926\n",
      "Epoch 5/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 0.5191 - rmse: 0.4367 - r_square: 0.9973 - val_loss: 1.7003 - val_rmse: 0.6712 - val_r_square: 0.9917\n",
      "Epoch 6/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 0.4572 - rmse: 0.4125 - r_square: 0.9976 - val_loss: 2.7064 - val_rmse: 0.8193 - val_r_square: 0.9867\n",
      "Epoch 7/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 0.4251 - rmse: 0.3979 - r_square: 0.9978 - val_loss: 1.8281 - val_rmse: 0.7315 - val_r_square: 0.9910\n",
      "Epoch 8/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 0.3998 - rmse: 0.3853 - r_square: 0.9979 - val_loss: 2.3602 - val_rmse: 0.7685 - val_r_square: 0.9884\n",
      "Epoch 9/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 0.3828 - rmse: 0.3777 - r_square: 0.9980 - val_loss: 2.2067 - val_rmse: 0.7325 - val_r_square: 0.9891\n",
      "Epoch 10/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3725 - rmse: 0.3722 - r_square: 0.9981 - val_loss: 2.5095 - val_rmse: 0.8494 - val_r_square: 0.9876\n",
      "Epoch 11/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3636 - rmse: 0.3676 - r_square: 0.9981 - val_loss: 2.3070 - val_rmse: 0.7931 - val_r_square: 0.9886\n",
      "Epoch 12/100\n",
      "1247590/1247590 [==============================] - 8s 6us/step - loss: 0.3514 - rmse: 0.3617 - r_square: 0.9982 - val_loss: 2.6403 - val_rmse: 0.7872 - val_r_square: 0.9870\n",
      "Epoch 13/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3461 - rmse: 0.3591 - r_square: 0.9982 - val_loss: 2.7071 - val_rmse: 0.8180 - val_r_square: 0.9867\n",
      "Epoch 14/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3409 - rmse: 0.3559 - r_square: 0.9982 - val_loss: 2.6415 - val_rmse: 0.7995 - val_r_square: 0.9870\n",
      "Epoch 15/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3348 - rmse: 0.3526 - r_square: 0.9983 - val_loss: 2.4289 - val_rmse: 0.7552 - val_r_square: 0.9880\n",
      "Epoch 16/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 0.3334 - rmse: 0.3514 - r_square: 0.9983 - val_loss: 2.6500 - val_rmse: 0.8036 - val_r_square: 0.9870\n",
      "Epoch 17/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3299 - rmse: 0.3497 - r_square: 0.9983 - val_loss: 2.9185 - val_rmse: 0.8612 - val_r_square: 0.9856\n",
      "Epoch 18/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3261 - rmse: 0.3477 - r_square: 0.9983 - val_loss: 2.8144 - val_rmse: 0.8288 - val_r_square: 0.9861\n",
      "Epoch 19/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3236 - rmse: 0.3460 - r_square: 0.9983 - val_loss: 3.2207 - val_rmse: 0.9109 - val_r_square: 0.9841\n",
      "Epoch 20/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3197 - rmse: 0.3442 - r_square: 0.9983 - val_loss: 2.7493 - val_rmse: 0.8229 - val_r_square: 0.9865\n",
      "Epoch 21/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3178 - rmse: 0.3428 - r_square: 0.9984 - val_loss: 3.2021 - val_rmse: 0.8650 - val_r_square: 0.9842\n",
      "Epoch 22/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3166 - rmse: 0.3425 - r_square: 0.9984 - val_loss: 3.1321 - val_rmse: 0.8698 - val_r_square: 0.9846\n",
      "Epoch 23/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3121 - rmse: 0.3393 - r_square: 0.9984 - val_loss: 3.0379 - val_rmse: 0.8644 - val_r_square: 0.9850\n",
      "Epoch 24/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3112 - rmse: 0.3387 - r_square: 0.9984 - val_loss: 3.0640 - val_rmse: 0.8612 - val_r_square: 0.9849\n",
      "Epoch 25/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3079 - rmse: 0.3371 - r_square: 0.9984 - val_loss: 2.8795 - val_rmse: 0.8403 - val_r_square: 0.9858\n",
      "Epoch 26/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 0.3065 - rmse: 0.3367 - r_square: 0.9984 - val_loss: 2.8004 - val_rmse: 0.8264 - val_r_square: 0.9862\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.1),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "model2.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")\n",
    "# fit the model\n",
    "result2 = model2.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 100,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3238823e-02]\n",
      " [9.9141829e-02]\n",
      " [9.3901718e-01]\n",
      " [2.3249245e+00]\n",
      " [1.9514221e-01]\n",
      " [1.1731058e+01]\n",
      " [2.7127060e+01]\n",
      " [8.1401424e+00]\n",
      " [8.2319689e+00]\n",
      " [4.4056717e+01]]\n",
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.824509\n",
      "Mean squared error (MSE):       2.804126\n",
      "Root mean squared error (RMSE): 1.674553\n",
      "R square (R^2):                 0.986366\n"
     ]
    }
   ],
   "source": [
    "predictions2 = model2.predict(X_test)\n",
    "print(predictions2[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions2))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions2))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions2)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try batch normalization technique to regularize neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1247590 samples, validate on 311898 samples\n",
      "Epoch 1/100\n",
      "1247590/1247590 [==============================] - 8s 6us/step - loss: 7.9399 - rmse: 1.2601 - r_square: 0.9605 - val_loss: 0.6608 - val_rmse: 0.5444 - val_r_square: 0.9966\n",
      "Epoch 2/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.4927 - rmse: 1.0205 - r_square: 0.9823 - val_loss: 1.1085 - val_rmse: 0.6601 - val_r_square: 0.9943\n",
      "Epoch 3/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.2452 - rmse: 0.9784 - r_square: 0.9837 - val_loss: 0.9889 - val_rmse: 0.5816 - val_r_square: 0.9949\n",
      "Epoch 4/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.3106 - rmse: 0.9762 - r_square: 0.9832 - val_loss: 0.9306 - val_rmse: 0.6041 - val_r_square: 0.9952\n",
      "Epoch 5/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.1642 - rmse: 0.9477 - r_square: 0.9839 - val_loss: 1.7566 - val_rmse: 0.7734 - val_r_square: 0.9910\n",
      "Epoch 6/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.1690 - rmse: 0.9471 - r_square: 0.9839 - val_loss: 0.7561 - val_rmse: 0.5119 - val_r_square: 0.9962\n",
      "Epoch 7/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.1032 - rmse: 0.9345 - r_square: 0.9843 - val_loss: 0.3948 - val_rmse: 0.4094 - val_r_square: 0.9980\n",
      "Epoch 8/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0714 - rmse: 0.9286 - r_square: 0.9844 - val_loss: 1.3250 - val_rmse: 0.6492 - val_r_square: 0.9932\n",
      "Epoch 9/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.1131 - rmse: 0.9313 - r_square: 0.9841 - val_loss: 0.4398 - val_rmse: 0.4060 - val_r_square: 0.9977\n",
      "Epoch 10/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0870 - rmse: 0.9237 - r_square: 0.9845 - val_loss: 1.0548 - val_rmse: 0.5813 - val_r_square: 0.9946\n",
      "Epoch 11/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0601 - rmse: 0.9204 - r_square: 0.9845 - val_loss: 0.5147 - val_rmse: 0.4200 - val_r_square: 0.9974\n",
      "Epoch 12/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0370 - rmse: 0.9188 - r_square: 0.9846 - val_loss: 0.3951 - val_rmse: 0.4113 - val_r_square: 0.9980\n",
      "Epoch 13/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9985 - rmse: 0.9129 - r_square: 0.9849 - val_loss: 0.4580 - val_rmse: 0.3979 - val_r_square: 0.9977\n",
      "Epoch 14/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0195 - rmse: 0.9160 - r_square: 0.9847 - val_loss: 0.3900 - val_rmse: 0.3924 - val_r_square: 0.9980\n",
      "Epoch 15/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9993 - rmse: 0.9095 - r_square: 0.9848 - val_loss: 0.9902 - val_rmse: 0.5824 - val_r_square: 0.9950\n",
      "Epoch 16/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9297 - rmse: 0.9029 - r_square: 0.9852 - val_loss: 0.6269 - val_rmse: 0.4838 - val_r_square: 0.9968\n",
      "Epoch 17/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0478 - rmse: 0.9178 - r_square: 0.9845 - val_loss: 0.5036 - val_rmse: 0.4202 - val_r_square: 0.9974\n",
      "Epoch 18/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9631 - rmse: 0.9081 - r_square: 0.9850 - val_loss: 0.4166 - val_rmse: 0.3753 - val_r_square: 0.9979\n",
      "Epoch 19/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0472 - rmse: 0.9135 - r_square: 0.9847 - val_loss: 0.3527 - val_rmse: 0.3787 - val_r_square: 0.9982\n",
      "Epoch 20/100\n",
      "1247590/1247590 [==============================] - 8s 6us/step - loss: 2.9593 - rmse: 0.9030 - r_square: 0.9850 - val_loss: 0.6343 - val_rmse: 0.4478 - val_r_square: 0.9968\n",
      "Epoch 21/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0322 - rmse: 0.9130 - r_square: 0.9846 - val_loss: 0.6821 - val_rmse: 0.5266 - val_r_square: 0.9965\n",
      "Epoch 22/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0166 - rmse: 0.9116 - r_square: 0.9848 - val_loss: 0.7044 - val_rmse: 0.4738 - val_r_square: 0.9964\n",
      "Epoch 23/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0271 - rmse: 0.9110 - r_square: 0.9846 - val_loss: 0.6022 - val_rmse: 0.4802 - val_r_square: 0.9969\n",
      "Epoch 24/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.8888 - rmse: 0.8935 - r_square: 0.9853 - val_loss: 0.3037 - val_rmse: 0.3388 - val_r_square: 0.9984\n",
      "Epoch 25/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0120 - rmse: 0.9092 - r_square: 0.9847 - val_loss: 0.4556 - val_rmse: 0.4004 - val_r_square: 0.9977\n",
      "Epoch 26/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 3.0524 - rmse: 0.9164 - r_square: 0.9845 - val_loss: 0.4858 - val_rmse: 0.4145 - val_r_square: 0.9975\n",
      "Epoch 27/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.8889 - rmse: 0.8948 - r_square: 0.9853 - val_loss: 0.6412 - val_rmse: 0.4665 - val_r_square: 0.9967\n",
      "Epoch 28/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.8385 - rmse: 0.8910 - r_square: 0.9855 - val_loss: 0.3566 - val_rmse: 0.3808 - val_r_square: 0.9982\n",
      "Epoch 29/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9416 - rmse: 0.9008 - r_square: 0.9851 - val_loss: 0.3061 - val_rmse: 0.3439 - val_r_square: 0.9984\n",
      "Epoch 30/100\n",
      "1247590/1247590 [==============================] - 8s 7us/step - loss: 2.9633 - rmse: 0.9004 - r_square: 0.9850 - val_loss: 0.4611 - val_rmse: 0.4330 - val_r_square: 0.9976\n",
      "Epoch 31/100\n",
      "1247590/1247590 [==============================] - 8s 7us/step - loss: 2.9320 - rmse: 0.8975 - r_square: 0.9852 - val_loss: 0.4694 - val_rmse: 0.4024 - val_r_square: 0.9976\n",
      "Epoch 32/100\n",
      "1247590/1247590 [==============================] - 8s 6us/step - loss: 2.9529 - rmse: 0.8999 - r_square: 0.9851 - val_loss: 0.4176 - val_rmse: 0.3996 - val_r_square: 0.9979\n",
      "Epoch 33/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.8896 - rmse: 0.8972 - r_square: 0.9853 - val_loss: 0.3157 - val_rmse: 0.3489 - val_r_square: 0.9984\n",
      "Epoch 34/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9389 - rmse: 0.8978 - r_square: 0.9851 - val_loss: 0.4313 - val_rmse: 0.4320 - val_r_square: 0.9978\n",
      "Epoch 35/100\n",
      "1247590/1247590 [==============================] - 8s 6us/step - loss: 2.9762 - rmse: 0.9016 - r_square: 0.9849 - val_loss: 0.3461 - val_rmse: 0.3691 - val_r_square: 0.9982\n",
      "Epoch 36/100\n",
      "1247590/1247590 [==============================] - 9s 7us/step - loss: 2.8108 - rmse: 0.8813 - r_square: 0.9857 - val_loss: 0.6048 - val_rmse: 0.4458 - val_r_square: 0.9969\n",
      "Epoch 37/100\n",
      "1247590/1247590 [==============================] - 8s 7us/step - loss: 2.8985 - rmse: 0.8927 - r_square: 0.9854 - val_loss: 0.3336 - val_rmse: 0.3585 - val_r_square: 0.9983\n",
      "Epoch 38/100\n",
      "1247590/1247590 [==============================] - 8s 6us/step - loss: 2.9356 - rmse: 0.8979 - r_square: 0.9850 - val_loss: 0.3538 - val_rmse: 0.3536 - val_r_square: 0.9982\n",
      "Epoch 39/100\n",
      "1247590/1247590 [==============================] - 8s 6us/step - loss: 2.9405 - rmse: 0.9004 - r_square: 0.9851 - val_loss: 0.5664 - val_rmse: 0.4384 - val_r_square: 0.9971\n",
      "Epoch 40/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9200 - rmse: 0.8958 - r_square: 0.9852 - val_loss: 0.3377 - val_rmse: 0.3583 - val_r_square: 0.9983\n",
      "Epoch 41/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9205 - rmse: 0.8974 - r_square: 0.9853 - val_loss: 0.8701 - val_rmse: 0.5377 - val_r_square: 0.9956\n",
      "Epoch 42/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.8793 - rmse: 0.8868 - r_square: 0.9855 - val_loss: 0.3420 - val_rmse: 0.3699 - val_r_square: 0.9982\n",
      "Epoch 43/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9084 - rmse: 0.8924 - r_square: 0.9852 - val_loss: 0.3584 - val_rmse: 0.3763 - val_r_square: 0.9982\n",
      "Epoch 44/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.8961 - rmse: 0.8918 - r_square: 0.9853 - val_loss: 0.6147 - val_rmse: 0.4509 - val_r_square: 0.9969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 2.9148 - rmse: 0.8928 - r_square: 0.9852 - val_loss: 0.6064 - val_rmse: 0.4566 - val_r_square: 0.9969\n",
      "Epoch 46/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9562 - rmse: 0.8957 - r_square: 0.9850 - val_loss: 0.3202 - val_rmse: 0.3359 - val_r_square: 0.9984\n",
      "Epoch 47/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9480 - rmse: 0.8964 - r_square: 0.9852 - val_loss: 0.3197 - val_rmse: 0.3468 - val_r_square: 0.9984\n",
      "Epoch 48/100\n",
      "1247590/1247590 [==============================] - 7s 6us/step - loss: 2.9550 - rmse: 0.8992 - r_square: 0.9851 - val_loss: 0.4523 - val_rmse: 0.4012 - val_r_square: 0.9977\n",
      "Epoch 49/100\n",
      "1247590/1247590 [==============================] - 7s 5us/step - loss: 2.9192 - rmse: 0.8952 - r_square: 0.9851 - val_loss: 0.3548 - val_rmse: 0.3626 - val_r_square: 0.9982\n",
      "Epoch 00049: early stopping\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential([\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "model3.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")\n",
    "# fit the model\n",
    "result3 = model3.fit(XX_train, \n",
    "                   yy_train.values,\n",
    "                   epochs =100,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation.values),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3048780e-02]\n",
      " [3.8709402e-02]\n",
      " [1.3599184e+00]\n",
      " [1.8800478e+00]\n",
      " [1.3280153e-02]\n",
      " [1.3064846e+01]\n",
      " [3.0613249e+01]\n",
      " [8.2126007e+00]\n",
      " [8.6445322e+00]\n",
      " [4.7861221e+01]]\n",
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.361444\n",
      "Mean squared error (MSE):       0.352840\n",
      "Root mean squared error (RMSE): 0.594003\n",
      "R square (R^2):                 0.998284\n"
     ]
    }
   ],
   "source": [
    "predictions3 = model3.predict(X_test)\n",
    "print(predictions3[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions3))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions3))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions3)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: \n",
    "&emsp;&emsp;What does 'Accuracy' mean in Regression? https://github.com/keras-team/keras/issues/7947 <br>\n",
    "&emsp;&emsp;https://keras.io/metrics/<br>\n",
    "&emsp;&emsp;Scale, Standardize, or Normalize with Scikit-Learn https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02 <br>\n",
    "&emsp;&emsp;The Day my Computer Won the Nobel Prize (Neural Network Option Pricing)  https://medium.com/datadriveninvestor/the-day-my-computer-won-the-nobel-prize-neural-network-option-pricing-d29b4379f1d2 <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
