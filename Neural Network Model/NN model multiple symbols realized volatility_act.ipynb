{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-19.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Requirement already satisfied: h5py in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: pyyaml in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.11.0)\n",
      "Installing collected packages: keras-applications, keras-preprocessing, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/4a/5c86ed8b245aa48f9f819b13a0a9039e9126ba19fdd0c7e0b8026c12315a/tensorflow-1.14.0-cp36-cp36m-win_amd64.whl (68.3MB)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/59/ec6c6075dd628f2efd33dbe11b259e63e1de7cbc26e45b38de767df528d4/protobuf-3.8.0-cp36-cp36m-win_amd64.whl (1.1MB)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/78/93/297ff3656f1073fba84e2f9633ad3b27a007eb59ad22099ac30142f80365/grpcio-1.22.0-cp36-cp36m-win_amd64.whl (1.6MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.1.0)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.0.8)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.1)\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/ed/e036d31a9b2c750f270cbb1cfc1c0f94ac78ae504eea7eec3267be4e294a/numpy-1.16.4-cp36-cp36m-win_amd64.whl (11.9MB)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: h5py in c:\\program files\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.1)\n",
      "Building wheels for collected packages: gast, termcolor, wrapt, absl-py\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\d7\\de\\2e\\efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\ee\\98\\38\\46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "Successfully built gast termcolor wrapt absl-py\n",
      "Installing collected packages: protobuf, gast, absl-py, markdown, grpcio, numpy, tensorboard, termcolor, wrapt, tensorflow-estimator, astor, google-pasta, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.22.0 markdown-3.1.1 numpy-1.16.4 protobuf-3.8.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 wrapt-1.11.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorboard 1.14.0 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.\n",
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts freeze_graph.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user keras\n",
    "!pip install --user tensorflow\n",
    "!pip install --user -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU only\n",
    "# Only Macbook needs to run this cell\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and do data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>exdate</th>\n",
       "      <th>maturity</th>\n",
       "      <th>strike_price</th>\n",
       "      <th>best_offer</th>\n",
       "      <th>volatility</th>\n",
       "      <th>underlying_price</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>cp_flag_C</th>\n",
       "      <th>cp_flag_P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.170513</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.170513</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.170513</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>46.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.170513</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>47.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.170513</td>\n",
       "      <td>58.75</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date      exdate  maturity  strike_price  best_offer  \\\n",
       "0    AXP  2013-01-02  2013-01-04         2          40.0        20.4   \n",
       "1    AXP  2013-01-02  2013-01-04         2          44.0        15.4   \n",
       "2    AXP  2013-01-02  2013-01-04         2          45.0        14.4   \n",
       "3    AXP  2013-01-02  2013-01-04         2          46.0        13.4   \n",
       "4    AXP  2013-01-02  2013-01-04         2          47.0        12.4   \n",
       "\n",
       "   volatility  underlying_price  interest_rate  cp_flag_C  cp_flag_P  \n",
       "0    0.170513             58.75         0.0008          1          0  \n",
       "1    0.170513             58.75         0.0008          1          0  \n",
       "2    0.170513             58.75         0.0008          1          0  \n",
       "3    0.170513             58.75         0.0008          1          0  \n",
       "4    0.170513             58.75         0.0008          1          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Options_R.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['volatility'].notnull()]\n",
    "y = df['best_offer'].values\n",
    "X = df[['maturity', 'strike_price', 'underlying_price', 'volatility', 'cp_flag_C', 'cp_flag_P', 'interest_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to build a regression neural network model\n",
    "sc= preprocessing.MinMaxScaler()\n",
    "X= sc.fit_transform(X)\n",
    "#y= y.reshape(-1,1)\n",
    "#y=sc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1079819, 7)\n",
      "(1079819,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train, XX_validation, yy_train, yy_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a keras Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0706 18:52:14.825369  3468 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0706 18:52:14.841377  3468 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0706 18:52:14.845366  3468 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Built Keras sequential model with 3 hidden layer, and after the first hidden layer.\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable early stopping based on the loss of validation data\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because it is a simple regression problem, we should custom metrics function\n",
    "from keras import backend as K\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis = -1))\n",
    "\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0706 18:52:24.525656  3468 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use mean_squared_error to compile regression model loss\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0706 18:52:32.725258  3468 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0706 18:52:32.816236  3468 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 863855 samples, validate on 215964 samples\n",
      "Epoch 1/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 16.7139 - rmse: 1.2588 - r_square: 0.9368 - val_loss: 0.3919 - val_rmse: 0.4234 - val_r_square: 0.9985\n",
      "Epoch 2/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.3667 - rmse: 0.3963 - r_square: 0.9985 - val_loss: 0.3103 - val_rmse: 0.3537 - val_r_square: 0.9988\n",
      "Epoch 3/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.3330 - rmse: 0.3758 - r_square: 0.9987 - val_loss: 0.4859 - val_rmse: 0.4964 - val_r_square: 0.9981\n",
      "Epoch 4/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.3113 - rmse: 0.3626 - r_square: 0.9988 - val_loss: 0.3155 - val_rmse: 0.3853 - val_r_square: 0.9988\n",
      "Epoch 5/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2997 - rmse: 0.3547 - r_square: 0.9988 - val_loss: 0.2917 - val_rmse: 0.3371 - val_r_square: 0.9988\n",
      "Epoch 6/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2887 - rmse: 0.3484 - r_square: 0.9989 - val_loss: 0.2696 - val_rmse: 0.3288 - val_r_square: 0.9989\n",
      "Epoch 7/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2827 - rmse: 0.3434 - r_square: 0.9989 - val_loss: 0.3232 - val_rmse: 0.3842 - val_r_square: 0.9987\n",
      "Epoch 8/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2749 - rmse: 0.3381 - r_square: 0.9989 - val_loss: 0.3033 - val_rmse: 0.3463 - val_r_square: 0.9988\n",
      "Epoch 9/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2743 - rmse: 0.3375 - r_square: 0.9989 - val_loss: 0.2466 - val_rmse: 0.3274 - val_r_square: 0.9990\n",
      "Epoch 10/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2692 - rmse: 0.3340 - r_square: 0.9989 - val_loss: 0.2921 - val_rmse: 0.3352 - val_r_square: 0.9988\n",
      "Epoch 11/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2652 - rmse: 0.3315 - r_square: 0.9989 - val_loss: 0.2404 - val_rmse: 0.3084 - val_r_square: 0.9991\n",
      "Epoch 12/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2603 - rmse: 0.3280 - r_square: 0.9990 - val_loss: 0.2349 - val_rmse: 0.3149 - val_r_square: 0.9991\n",
      "Epoch 13/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2567 - rmse: 0.3255 - r_square: 0.9990 - val_loss: 0.2640 - val_rmse: 0.3235 - val_r_square: 0.9990\n",
      "Epoch 14/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2566 - rmse: 0.3251 - r_square: 0.9990 - val_loss: 0.2776 - val_rmse: 0.3545 - val_r_square: 0.9989\n",
      "Epoch 15/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2536 - rmse: 0.3233 - r_square: 0.9990 - val_loss: 0.2423 - val_rmse: 0.3110 - val_r_square: 0.9990\n",
      "Epoch 16/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2505 - rmse: 0.3211 - r_square: 0.9990 - val_loss: 0.3203 - val_rmse: 0.3728 - val_r_square: 0.9987\n",
      "Epoch 17/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2469 - rmse: 0.3184 - r_square: 0.9990 - val_loss: 0.2289 - val_rmse: 0.3062 - val_r_square: 0.9991\n",
      "Epoch 18/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2474 - rmse: 0.3186 - r_square: 0.9990 - val_loss: 0.2516 - val_rmse: 0.3150 - val_r_square: 0.9990\n",
      "Epoch 19/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2449 - rmse: 0.3168 - r_square: 0.9990 - val_loss: 0.2268 - val_rmse: 0.3086 - val_r_square: 0.9991\n",
      "Epoch 20/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2423 - rmse: 0.3150 - r_square: 0.9990 - val_loss: 0.2595 - val_rmse: 0.3118 - val_r_square: 0.9990\n",
      "Epoch 21/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2405 - rmse: 0.3132 - r_square: 0.9990 - val_loss: 0.2386 - val_rmse: 0.3148 - val_r_square: 0.9991\n",
      "Epoch 22/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2380 - rmse: 0.3117 - r_square: 0.9991 - val_loss: 0.2605 - val_rmse: 0.3343 - val_r_square: 0.9990\n",
      "Epoch 23/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2383 - rmse: 0.3118 - r_square: 0.9991 - val_loss: 0.2685 - val_rmse: 0.3545 - val_r_square: 0.9989\n",
      "Epoch 24/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2341 - rmse: 0.3093 - r_square: 0.9991 - val_loss: 0.3348 - val_rmse: 0.3581 - val_r_square: 0.9987\n",
      "Epoch 25/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2357 - rmse: 0.3102 - r_square: 0.9991 - val_loss: 0.2963 - val_rmse: 0.3441 - val_r_square: 0.9988\n",
      "Epoch 26/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2315 - rmse: 0.3075 - r_square: 0.9991 - val_loss: 0.2074 - val_rmse: 0.2934 - val_r_square: 0.9992\n",
      "Epoch 27/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2311 - rmse: 0.3070 - r_square: 0.9991 - val_loss: 0.2104 - val_rmse: 0.2894 - val_r_square: 0.9992\n",
      "Epoch 28/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2289 - rmse: 0.3053 - r_square: 0.9991 - val_loss: 0.2255 - val_rmse: 0.3054 - val_r_square: 0.9991\n",
      "Epoch 29/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2295 - rmse: 0.3059 - r_square: 0.9991 - val_loss: 0.2130 - val_rmse: 0.3013 - val_r_square: 0.9992\n",
      "Epoch 30/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2272 - rmse: 0.3044 - r_square: 0.9991 - val_loss: 0.2994 - val_rmse: 0.3328 - val_r_square: 0.9988\n",
      "Epoch 31/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2243 - rmse: 0.3023 - r_square: 0.9991 - val_loss: 0.2124 - val_rmse: 0.2938 - val_r_square: 0.9992\n",
      "Epoch 32/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2226 - rmse: 0.3013 - r_square: 0.9991 - val_loss: 0.2173 - val_rmse: 0.2950 - val_r_square: 0.9991\n",
      "Epoch 33/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2231 - rmse: 0.3020 - r_square: 0.9991 - val_loss: 0.2776 - val_rmse: 0.3530 - val_r_square: 0.9989\n",
      "Epoch 34/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2212 - rmse: 0.3003 - r_square: 0.9991 - val_loss: 0.2866 - val_rmse: 0.3579 - val_r_square: 0.9989\n",
      "Epoch 35/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2197 - rmse: 0.2995 - r_square: 0.9991 - val_loss: 0.2080 - val_rmse: 0.2934 - val_r_square: 0.9992\n",
      "Epoch 36/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2195 - rmse: 0.2995 - r_square: 0.9991 - val_loss: 0.2616 - val_rmse: 0.3424 - val_r_square: 0.9990\n",
      "Epoch 37/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2176 - rmse: 0.2980 - r_square: 0.9991 - val_loss: 0.2440 - val_rmse: 0.3261 - val_r_square: 0.9990\n",
      "Epoch 38/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2178 - rmse: 0.2981 - r_square: 0.9991 - val_loss: 0.2045 - val_rmse: 0.2819 - val_r_square: 0.9992\n",
      "Epoch 39/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2166 - rmse: 0.2974 - r_square: 0.9991 - val_loss: 0.2088 - val_rmse: 0.2820 - val_r_square: 0.9992\n",
      "Epoch 40/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2159 - rmse: 0.2968 - r_square: 0.9991 - val_loss: 0.2044 - val_rmse: 0.2779 - val_r_square: 0.9992\n",
      "Epoch 41/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2156 - rmse: 0.2963 - r_square: 0.9991 - val_loss: 0.2381 - val_rmse: 0.3129 - val_r_square: 0.9991\n",
      "Epoch 42/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2138 - rmse: 0.2950 - r_square: 0.9992 - val_loss: 0.2086 - val_rmse: 0.2865 - val_r_square: 0.9992\n",
      "Epoch 43/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2126 - rmse: 0.2942 - r_square: 0.9992 - val_loss: 0.2989 - val_rmse: 0.3349 - val_r_square: 0.9988\n",
      "Epoch 44/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2145 - rmse: 0.2955 - r_square: 0.9991 - val_loss: 0.1984 - val_rmse: 0.2844 - val_r_square: 0.9992\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2136 - rmse: 0.2951 - r_square: 0.9992 - val_loss: 0.2277 - val_rmse: 0.3286 - val_r_square: 0.9991\n",
      "Epoch 46/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2127 - rmse: 0.2946 - r_square: 0.9992 - val_loss: 0.2141 - val_rmse: 0.2868 - val_r_square: 0.9992\n",
      "Epoch 47/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2111 - rmse: 0.2930 - r_square: 0.9992 - val_loss: 0.2045 - val_rmse: 0.2867 - val_r_square: 0.9992\n",
      "Epoch 48/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2115 - rmse: 0.2934 - r_square: 0.9992 - val_loss: 0.2152 - val_rmse: 0.3096 - val_r_square: 0.9992\n",
      "Epoch 49/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2090 - rmse: 0.2914 - r_square: 0.9992 - val_loss: 0.2047 - val_rmse: 0.2955 - val_r_square: 0.9992\n",
      "Epoch 50/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2096 - rmse: 0.2918 - r_square: 0.9992 - val_loss: 0.2171 - val_rmse: 0.2944 - val_r_square: 0.9991\n",
      "Epoch 51/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2100 - rmse: 0.2922 - r_square: 0.9992 - val_loss: 0.2375 - val_rmse: 0.3220 - val_r_square: 0.9991\n",
      "Epoch 52/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2076 - rmse: 0.2904 - r_square: 0.9992 - val_loss: 0.2368 - val_rmse: 0.3161 - val_r_square: 0.9991\n",
      "Epoch 53/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2087 - rmse: 0.2910 - r_square: 0.9992 - val_loss: 0.1908 - val_rmse: 0.2761 - val_r_square: 0.9992\n",
      "Epoch 54/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2055 - rmse: 0.2887 - r_square: 0.9992 - val_loss: 0.2238 - val_rmse: 0.3038 - val_r_square: 0.9991\n",
      "Epoch 55/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2085 - rmse: 0.2905 - r_square: 0.9992 - val_loss: 0.2371 - val_rmse: 0.3090 - val_r_square: 0.9991\n",
      "Epoch 56/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2063 - rmse: 0.2891 - r_square: 0.9992 - val_loss: 0.2031 - val_rmse: 0.2798 - val_r_square: 0.9992\n",
      "Epoch 57/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2060 - rmse: 0.2890 - r_square: 0.9992 - val_loss: 0.2162 - val_rmse: 0.2902 - val_r_square: 0.9991\n",
      "Epoch 58/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2061 - rmse: 0.2889 - r_square: 0.9992 - val_loss: 0.1925 - val_rmse: 0.2760 - val_r_square: 0.9992\n",
      "Epoch 59/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2043 - rmse: 0.2876 - r_square: 0.9992 - val_loss: 0.2859 - val_rmse: 0.3219 - val_r_square: 0.9989\n",
      "Epoch 60/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2043 - rmse: 0.2875 - r_square: 0.9992 - val_loss: 0.2280 - val_rmse: 0.3120 - val_r_square: 0.9991\n",
      "Epoch 61/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2043 - rmse: 0.2875 - r_square: 0.9992 - val_loss: 0.1926 - val_rmse: 0.2784 - val_r_square: 0.9992\n",
      "Epoch 62/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2034 - rmse: 0.2869 - r_square: 0.9992 - val_loss: 0.1846 - val_rmse: 0.2694 - val_r_square: 0.9993\n",
      "Epoch 63/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2037 - rmse: 0.2870 - r_square: 0.9992 - val_loss: 0.2155 - val_rmse: 0.3035 - val_r_square: 0.9991\n",
      "Epoch 64/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2030 - rmse: 0.2862 - r_square: 0.9992 - val_loss: 0.2182 - val_rmse: 0.3090 - val_r_square: 0.9991\n",
      "Epoch 65/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2027 - rmse: 0.2865 - r_square: 0.9992 - val_loss: 0.1904 - val_rmse: 0.2715 - val_r_square: 0.9992\n",
      "Epoch 66/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2020 - rmse: 0.2856 - r_square: 0.9992 - val_loss: 0.1895 - val_rmse: 0.2689 - val_r_square: 0.9993\n",
      "Epoch 67/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2026 - rmse: 0.2861 - r_square: 0.9992 - val_loss: 0.2160 - val_rmse: 0.2996 - val_r_square: 0.9992\n",
      "Epoch 68/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2011 - rmse: 0.2850 - r_square: 0.9992 - val_loss: 0.2110 - val_rmse: 0.2849 - val_r_square: 0.9992\n",
      "Epoch 69/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2016 - rmse: 0.2852 - r_square: 0.9992 - val_loss: 0.1885 - val_rmse: 0.2824 - val_r_square: 0.9993\n",
      "Epoch 70/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2020 - rmse: 0.2856 - r_square: 0.9992 - val_loss: 0.1967 - val_rmse: 0.2820 - val_r_square: 0.9992\n",
      "Epoch 71/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2000 - rmse: 0.2841 - r_square: 0.9992 - val_loss: 0.2134 - val_rmse: 0.2919 - val_r_square: 0.9992\n",
      "Epoch 72/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.2012 - rmse: 0.2848 - r_square: 0.9992 - val_loss: 0.1906 - val_rmse: 0.2686 - val_r_square: 0.9992\n",
      "Epoch 73/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1993 - rmse: 0.2832 - r_square: 0.9992 - val_loss: 0.1892 - val_rmse: 0.2765 - val_r_square: 0.9993\n",
      "Epoch 74/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1992 - rmse: 0.2834 - r_square: 0.9992 - val_loss: 0.2017 - val_rmse: 0.3001 - val_r_square: 0.9992\n",
      "Epoch 75/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1991 - rmse: 0.2834 - r_square: 0.9992 - val_loss: 0.1945 - val_rmse: 0.2753 - val_r_square: 0.9992\n",
      "Epoch 76/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1995 - rmse: 0.2839 - r_square: 0.9992 - val_loss: 0.1992 - val_rmse: 0.2891 - val_r_square: 0.9992\n",
      "Epoch 77/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1989 - rmse: 0.2830 - r_square: 0.9992 - val_loss: 0.1985 - val_rmse: 0.2819 - val_r_square: 0.9992\n",
      "Epoch 78/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1985 - rmse: 0.2828 - r_square: 0.9992 - val_loss: 0.2049 - val_rmse: 0.2921 - val_r_square: 0.9992\n",
      "Epoch 79/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1971 - rmse: 0.2817 - r_square: 0.9992 - val_loss: 0.2051 - val_rmse: 0.2988 - val_r_square: 0.9992\n",
      "Epoch 80/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1983 - rmse: 0.2827 - r_square: 0.9992 - val_loss: 0.1874 - val_rmse: 0.2717 - val_r_square: 0.9993\n",
      "Epoch 81/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1985 - rmse: 0.2829 - r_square: 0.9992 - val_loss: 0.2128 - val_rmse: 0.3071 - val_r_square: 0.9992\n",
      "Epoch 82/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1968 - rmse: 0.2814 - r_square: 0.9992 - val_loss: 0.2016 - val_rmse: 0.2971 - val_r_square: 0.9992\n",
      "Epoch 83/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1970 - rmse: 0.2818 - r_square: 0.9992 - val_loss: 0.2188 - val_rmse: 0.2868 - val_r_square: 0.9991\n",
      "Epoch 84/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1973 - rmse: 0.2820 - r_square: 0.9992 - val_loss: 0.2103 - val_rmse: 0.2876 - val_r_square: 0.9992\n",
      "Epoch 85/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1966 - rmse: 0.2814 - r_square: 0.9992 - val_loss: 0.2145 - val_rmse: 0.3081 - val_r_square: 0.9992\n",
      "Epoch 86/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1962 - rmse: 0.2813 - r_square: 0.9992 - val_loss: 0.2147 - val_rmse: 0.2982 - val_r_square: 0.9992\n",
      "Epoch 87/100\n",
      "863855/863855 [==============================] - 4s 5us/step - loss: 0.1945 - rmse: 0.2797 - r_square: 0.9992 - val_loss: 0.2286 - val_rmse: 0.3215 - val_r_square: 0.9991\n",
      "Epoch 00087: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "result = model.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 100,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      " [[ 0.7452288 ]\n",
      " [ 8.072299  ]\n",
      " [44.38583   ]\n",
      " [13.630843  ]\n",
      " [ 1.9488716 ]\n",
      " [ 0.08636543]\n",
      " [ 6.305229  ]\n",
      " [ 4.6770115 ]\n",
      " [ 9.619084  ]\n",
      " [18.397778  ]]\n",
      "Actual:\n",
      " [3.400e-01 7.600e+00 4.435e+01 1.425e+01 1.760e+00 3.000e-02 5.500e+00\n",
      " 4.500e+00 9.350e+00 1.795e+01]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print('Prediction:\\n', predictions[:10])\n",
    "print('Actual:\\n', y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curves including R^2 and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training curve for rmse\n",
    "plt.plot(result.history['rmse'])\n",
    "plt.plot(result.history['val_rmse'])\n",
    "plt.title('rmse')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training curve for R^2\n",
    "plt.plot(result.history['r_square'])\n",
    "plt.plot(result.history['val_r_square'])\n",
    "plt.title('model R^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.321028\n",
      "Mean squared error (MSE):       0.227366\n",
      "Root mean squared error (RMSE): 0.476829\n",
      "R square (R^2):                 0.999139\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as skm, math\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0706 19:00:26.179871  3468 deprecation.py:506] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 863855 samples, validate on 215964 samples\n",
      "Epoch 1/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 20.4560 - rmse: 1.8398 - r_square: 0.9218 - val_loss: 0.6476 - val_rmse: 0.4984 - val_r_square: 0.9975\n",
      "Epoch 2/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 1.5154 - rmse: 0.7344 - r_square: 0.9941 - val_loss: 1.0527 - val_rmse: 0.6566 - val_r_square: 0.9959\n",
      "Epoch 3/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.9880 - rmse: 0.6105 - r_square: 0.9961 - val_loss: 2.0193 - val_rmse: 0.7998 - val_r_square: 0.9923\n",
      "Epoch 4/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.7364 - rmse: 0.5351 - r_square: 0.9971 - val_loss: 1.8877 - val_rmse: 0.7485 - val_r_square: 0.9928\n",
      "Epoch 5/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.5961 - rmse: 0.4862 - r_square: 0.9976 - val_loss: 2.9258 - val_rmse: 0.9831 - val_r_square: 0.9888\n",
      "Epoch 6/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.5079 - rmse: 0.4534 - r_square: 0.9980 - val_loss: 2.7706 - val_rmse: 0.9060 - val_r_square: 0.9895\n",
      "Epoch 7/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.4594 - rmse: 0.4327 - r_square: 0.9982 - val_loss: 3.5005 - val_rmse: 0.9953 - val_r_square: 0.9867\n",
      "Epoch 8/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.4260 - rmse: 0.4170 - r_square: 0.9983 - val_loss: 3.2592 - val_rmse: 0.9487 - val_r_square: 0.9876\n",
      "Epoch 9/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3951 - rmse: 0.4028 - r_square: 0.9984 - val_loss: 3.5815 - val_rmse: 0.9847 - val_r_square: 0.9864\n",
      "Epoch 10/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3773 - rmse: 0.3950 - r_square: 0.9985 - val_loss: 2.5586 - val_rmse: 0.8552 - val_r_square: 0.9903\n",
      "Epoch 11/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3621 - rmse: 0.3867 - r_square: 0.9986 - val_loss: 3.7761 - val_rmse: 1.0496 - val_r_square: 0.9856\n",
      "Epoch 12/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3515 - rmse: 0.3813 - r_square: 0.9986 - val_loss: 3.3080 - val_rmse: 0.9744 - val_r_square: 0.9874\n",
      "Epoch 13/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3428 - rmse: 0.3757 - r_square: 0.9986 - val_loss: 3.2424 - val_rmse: 0.9440 - val_r_square: 0.9877\n",
      "Epoch 14/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3322 - rmse: 0.3708 - r_square: 0.9987 - val_loss: 3.3687 - val_rmse: 0.9874 - val_r_square: 0.9872\n",
      "Epoch 15/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3287 - rmse: 0.3684 - r_square: 0.9987 - val_loss: 4.4914 - val_rmse: 1.1786 - val_r_square: 0.9828\n",
      "Epoch 16/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3220 - rmse: 0.3655 - r_square: 0.9987 - val_loss: 3.2369 - val_rmse: 0.9609 - val_r_square: 0.9877\n",
      "Epoch 17/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3156 - rmse: 0.3611 - r_square: 0.9987 - val_loss: 3.6673 - val_rmse: 1.0537 - val_r_square: 0.9860\n",
      "Epoch 18/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3146 - rmse: 0.3601 - r_square: 0.9987 - val_loss: 4.2934 - val_rmse: 1.1667 - val_r_square: 0.9836\n",
      "Epoch 19/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3081 - rmse: 0.3572 - r_square: 0.9988 - val_loss: 4.0172 - val_rmse: 1.0525 - val_r_square: 0.9847\n",
      "Epoch 20/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3057 - rmse: 0.3553 - r_square: 0.9988 - val_loss: 4.0503 - val_rmse: 1.0814 - val_r_square: 0.9845\n",
      "Epoch 21/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.3014 - rmse: 0.3530 - r_square: 0.9988 - val_loss: 3.8210 - val_rmse: 1.0438 - val_r_square: 0.9854\n",
      "Epoch 22/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.2997 - rmse: 0.3516 - r_square: 0.9988 - val_loss: 3.1725 - val_rmse: 0.8896 - val_r_square: 0.9879\n",
      "Epoch 23/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.2983 - rmse: 0.3504 - r_square: 0.9988 - val_loss: 4.1341 - val_rmse: 1.0906 - val_r_square: 0.9842\n",
      "Epoch 24/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.2913 - rmse: 0.3472 - r_square: 0.9988 - val_loss: 4.0889 - val_rmse: 1.0618 - val_r_square: 0.9844\n",
      "Epoch 25/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.2894 - rmse: 0.3458 - r_square: 0.9988 - val_loss: 3.8881 - val_rmse: 1.0674 - val_r_square: 0.9852\n",
      "Epoch 26/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 0.2888 - rmse: 0.3454 - r_square: 0.9989 - val_loss: 3.9983 - val_rmse: 1.0919 - val_r_square: 0.9847\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dropout(0.1),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "model2.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")\n",
    "# fit the model\n",
    "result2 = model2.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 100,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.96851486]\n",
      " [ 6.4468536 ]\n",
      " [40.347363  ]\n",
      " [11.446528  ]\n",
      " [ 1.8761499 ]\n",
      " [ 0.12979294]\n",
      " [ 4.9536304 ]\n",
      " [ 4.142134  ]\n",
      " [ 8.117151  ]\n",
      " [16.198263  ]]\n",
      "\n",
      "\n",
      "Mean absolute error (MAE):      1.091110\n",
      "Mean squared error (MSE):       3.988861\n",
      "Root mean squared error (RMSE): 1.997213\n",
      "R square (R^2):                 0.984898\n"
     ]
    }
   ],
   "source": [
    "predictions2 = model2.predict(X_test)\n",
    "print(predictions2[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions2))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions2))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions2)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try batch normalization technique to regularize neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 863855 samples, validate on 215964 samples\n",
      "Epoch 1/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 13.3392 - rmse: 1.6708 - r_square: 0.9485 - val_loss: 0.8619 - val_rmse: 0.6012 - val_r_square: 0.9966\n",
      "Epoch 2/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 4.2276 - rmse: 1.1703 - r_square: 0.9835 - val_loss: 0.4646 - val_rmse: 0.4274 - val_r_square: 0.9982\n",
      "Epoch 3/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 4.0410 - rmse: 1.1328 - r_square: 0.9842 - val_loss: 0.5685 - val_rmse: 0.4942 - val_r_square: 0.9978\n",
      "Epoch 4/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 4.0703 - rmse: 1.1338 - r_square: 0.9840 - val_loss: 0.6028 - val_rmse: 0.5206 - val_r_square: 0.9976\n",
      "Epoch 5/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.8175 - rmse: 1.0957 - r_square: 0.9851 - val_loss: 0.7584 - val_rmse: 0.5792 - val_r_square: 0.9970\n",
      "Epoch 6/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.8404 - rmse: 1.0964 - r_square: 0.9850 - val_loss: 0.4596 - val_rmse: 0.4073 - val_r_square: 0.9982\n",
      "Epoch 7/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.8380 - rmse: 1.0941 - r_square: 0.9849 - val_loss: 0.4074 - val_rmse: 0.4137 - val_r_square: 0.9984\n",
      "Epoch 8/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.8733 - rmse: 1.0965 - r_square: 0.9849 - val_loss: 0.5369 - val_rmse: 0.4634 - val_r_square: 0.9979\n",
      "Epoch 9/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.8366 - rmse: 1.0960 - r_square: 0.9849 - val_loss: 0.5750 - val_rmse: 0.4409 - val_r_square: 0.9978\n",
      "Epoch 10/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.8406 - rmse: 1.0912 - r_square: 0.9850 - val_loss: 0.6914 - val_rmse: 0.5598 - val_r_square: 0.9973\n",
      "Epoch 11/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.7662 - rmse: 1.0734 - r_square: 0.9852 - val_loss: 0.8234 - val_rmse: 0.6002 - val_r_square: 0.9968\n",
      "Epoch 12/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6986 - rmse: 1.0700 - r_square: 0.9854 - val_loss: 0.3989 - val_rmse: 0.3993 - val_r_square: 0.9984\n",
      "Epoch 13/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.7382 - rmse: 1.0729 - r_square: 0.9854 - val_loss: 0.6480 - val_rmse: 0.4871 - val_r_square: 0.9975\n",
      "Epoch 14/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6157 - rmse: 1.0591 - r_square: 0.9858 - val_loss: 1.4115 - val_rmse: 0.7636 - val_r_square: 0.9945\n",
      "Epoch 15/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.8051 - rmse: 1.0864 - r_square: 0.9852 - val_loss: 0.3908 - val_rmse: 0.3921 - val_r_square: 0.9985\n",
      "Epoch 16/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.7244 - rmse: 1.0683 - r_square: 0.9854 - val_loss: 0.4405 - val_rmse: 0.4481 - val_r_square: 0.9983\n",
      "Epoch 17/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6211 - rmse: 1.0548 - r_square: 0.9858 - val_loss: 0.4277 - val_rmse: 0.4109 - val_r_square: 0.9983\n",
      "Epoch 18/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.7206 - rmse: 1.0693 - r_square: 0.9854 - val_loss: 1.0763 - val_rmse: 0.6250 - val_r_square: 0.9958\n",
      "Epoch 19/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.7480 - rmse: 1.0682 - r_square: 0.9853 - val_loss: 0.6777 - val_rmse: 0.5359 - val_r_square: 0.9974\n",
      "Epoch 20/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5881 - rmse: 1.0531 - r_square: 0.9859 - val_loss: 0.3641 - val_rmse: 0.4058 - val_r_square: 0.9986\n",
      "Epoch 21/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6754 - rmse: 1.0616 - r_square: 0.9856 - val_loss: 0.7014 - val_rmse: 0.4976 - val_r_square: 0.9973\n",
      "Epoch 22/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6813 - rmse: 1.0615 - r_square: 0.9856 - val_loss: 1.2155 - val_rmse: 0.6371 - val_r_square: 0.9953\n",
      "Epoch 23/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6186 - rmse: 1.0569 - r_square: 0.9858 - val_loss: 0.6922 - val_rmse: 0.4894 - val_r_square: 0.9973\n",
      "Epoch 24/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6168 - rmse: 1.0582 - r_square: 0.9858 - val_loss: 0.3802 - val_rmse: 0.3909 - val_r_square: 0.9985\n",
      "Epoch 25/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.8821 - rmse: 1.0895 - r_square: 0.9848 - val_loss: 0.4273 - val_rmse: 0.4155 - val_r_square: 0.9983\n",
      "Epoch 26/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.7147 - rmse: 1.0637 - r_square: 0.9855 - val_loss: 0.4201 - val_rmse: 0.3964 - val_r_square: 0.9984\n",
      "Epoch 27/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5474 - rmse: 1.0417 - r_square: 0.9862 - val_loss: 0.9463 - val_rmse: 0.5623 - val_r_square: 0.9963\n",
      "Epoch 28/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5802 - rmse: 1.0463 - r_square: 0.9859 - val_loss: 0.6447 - val_rmse: 0.4622 - val_r_square: 0.9975\n",
      "Epoch 29/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6792 - rmse: 1.0596 - r_square: 0.9856 - val_loss: 0.5202 - val_rmse: 0.4523 - val_r_square: 0.9980\n",
      "Epoch 30/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6380 - rmse: 1.0562 - r_square: 0.9858 - val_loss: 0.5599 - val_rmse: 0.4940 - val_r_square: 0.9978\n",
      "Epoch 31/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5666 - rmse: 1.0476 - r_square: 0.9861 - val_loss: 0.5587 - val_rmse: 0.4576 - val_r_square: 0.9978\n",
      "Epoch 32/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6524 - rmse: 1.0619 - r_square: 0.9856 - val_loss: 0.3433 - val_rmse: 0.3695 - val_r_square: 0.9987\n",
      "Epoch 33/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6470 - rmse: 1.0599 - r_square: 0.9857 - val_loss: 0.5743 - val_rmse: 0.4693 - val_r_square: 0.9978\n",
      "Epoch 34/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6621 - rmse: 1.0602 - r_square: 0.9857 - val_loss: 0.4205 - val_rmse: 0.3984 - val_r_square: 0.9984\n",
      "Epoch 35/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6465 - rmse: 1.0552 - r_square: 0.9857 - val_loss: 0.8434 - val_rmse: 0.5235 - val_r_square: 0.9967\n",
      "Epoch 36/100\n",
      "863855/863855 [==============================] - 5s 5us/step - loss: 3.6456 - rmse: 1.0621 - r_square: 0.9857 - val_loss: 0.4141 - val_rmse: 0.4085 - val_r_square: 0.9984\n",
      "Epoch 37/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5434 - rmse: 1.0415 - r_square: 0.9862 - val_loss: 0.4496 - val_rmse: 0.4187 - val_r_square: 0.9982\n",
      "Epoch 38/100\n",
      "863855/863855 [==============================] - 5s 5us/step - loss: 3.6410 - rmse: 1.0516 - r_square: 0.9858 - val_loss: 0.6889 - val_rmse: 0.4623 - val_r_square: 0.9973\n",
      "Epoch 39/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5991 - rmse: 1.0470 - r_square: 0.9860 - val_loss: 0.6172 - val_rmse: 0.4787 - val_r_square: 0.9976\n",
      "Epoch 40/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5134 - rmse: 1.0400 - r_square: 0.9862 - val_loss: 0.4769 - val_rmse: 0.4379 - val_r_square: 0.9981\n",
      "Epoch 41/100\n",
      "863855/863855 [==============================] - 5s 5us/step - loss: 3.5291 - rmse: 1.0413 - r_square: 0.9861 - val_loss: 0.4498 - val_rmse: 0.4282 - val_r_square: 0.9982\n",
      "Epoch 42/100\n",
      "863855/863855 [==============================] - 5s 5us/step - loss: 3.5233 - rmse: 1.0419 - r_square: 0.9862 - val_loss: 0.5937 - val_rmse: 0.4320 - val_r_square: 0.9977\n",
      "Epoch 43/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6267 - rmse: 1.0529 - r_square: 0.9858 - val_loss: 0.3609 - val_rmse: 0.3895 - val_r_square: 0.9986\n",
      "Epoch 44/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6300 - rmse: 1.0554 - r_square: 0.9858 - val_loss: 0.4636 - val_rmse: 0.4091 - val_r_square: 0.9982\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6306 - rmse: 1.0592 - r_square: 0.9858 - val_loss: 0.3527 - val_rmse: 0.3842 - val_r_square: 0.9986\n",
      "Epoch 46/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6800 - rmse: 1.0599 - r_square: 0.9856 - val_loss: 0.4159 - val_rmse: 0.4220 - val_r_square: 0.9984\n",
      "Epoch 47/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5382 - rmse: 1.0404 - r_square: 0.9861 - val_loss: 1.0692 - val_rmse: 0.6028 - val_r_square: 0.9959\n",
      "Epoch 48/100\n",
      "863855/863855 [==============================] - 5s 5us/step - loss: 3.6088 - rmse: 1.0512 - r_square: 0.9859 - val_loss: 0.3410 - val_rmse: 0.3686 - val_r_square: 0.9987\n",
      "Epoch 49/100\n",
      "863855/863855 [==============================] - 5s 5us/step - loss: 3.5764 - rmse: 1.0479 - r_square: 0.9860 - val_loss: 0.7133 - val_rmse: 0.6001 - val_r_square: 0.9972\n",
      "Epoch 50/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5432 - rmse: 1.0447 - r_square: 0.9862 - val_loss: 0.4090 - val_rmse: 0.3852 - val_r_square: 0.9984\n",
      "Epoch 51/100\n",
      "863855/863855 [==============================] - 5s 5us/step - loss: 3.5758 - rmse: 1.0457 - r_square: 0.9860 - val_loss: 0.4574 - val_rmse: 0.4306 - val_r_square: 0.9982\n",
      "Epoch 52/100\n",
      "863855/863855 [==============================] - 5s 5us/step - loss: 3.5867 - rmse: 1.0460 - r_square: 0.9860 - val_loss: 0.3660 - val_rmse: 0.3714 - val_r_square: 0.9986\n",
      "Epoch 53/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5775 - rmse: 1.0453 - r_square: 0.9860 - val_loss: 0.5592 - val_rmse: 0.4500 - val_r_square: 0.9978\n",
      "Epoch 54/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4398 - rmse: 1.0279 - r_square: 0.9865 - val_loss: 0.4702 - val_rmse: 0.3959 - val_r_square: 0.9982\n",
      "Epoch 55/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4225 - rmse: 1.0235 - r_square: 0.9866 - val_loss: 0.5203 - val_rmse: 0.4448 - val_r_square: 0.9980\n",
      "Epoch 56/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5256 - rmse: 1.0370 - r_square: 0.9862 - val_loss: 0.4965 - val_rmse: 0.4158 - val_r_square: 0.9981\n",
      "Epoch 57/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.6599 - rmse: 1.0530 - r_square: 0.9857 - val_loss: 0.4586 - val_rmse: 0.4196 - val_r_square: 0.9982\n",
      "Epoch 58/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4673 - rmse: 1.0321 - r_square: 0.9865 - val_loss: 0.4147 - val_rmse: 0.3878 - val_r_square: 0.9984\n",
      "Epoch 59/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5173 - rmse: 1.0341 - r_square: 0.9862 - val_loss: 0.4541 - val_rmse: 0.3995 - val_r_square: 0.9982\n",
      "Epoch 60/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4532 - rmse: 1.0306 - r_square: 0.9864 - val_loss: 0.4602 - val_rmse: 0.4326 - val_r_square: 0.9982\n",
      "Epoch 61/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4642 - rmse: 1.0312 - r_square: 0.9864 - val_loss: 0.7786 - val_rmse: 0.4951 - val_r_square: 0.9970\n",
      "Epoch 62/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4766 - rmse: 1.0271 - r_square: 0.9864 - val_loss: 0.3916 - val_rmse: 0.3975 - val_r_square: 0.9985\n",
      "Epoch 63/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4905 - rmse: 1.0342 - r_square: 0.9863 - val_loss: 0.4790 - val_rmse: 0.4222 - val_r_square: 0.9981\n",
      "Epoch 64/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4432 - rmse: 1.0282 - r_square: 0.9865 - val_loss: 0.5842 - val_rmse: 0.4549 - val_r_square: 0.9977\n",
      "Epoch 65/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4277 - rmse: 1.0284 - r_square: 0.9865 - val_loss: 0.5903 - val_rmse: 0.4379 - val_r_square: 0.9977\n",
      "Epoch 66/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5399 - rmse: 1.0405 - r_square: 0.9862 - val_loss: 0.5626 - val_rmse: 0.4496 - val_r_square: 0.9978\n",
      "Epoch 67/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4192 - rmse: 1.0271 - r_square: 0.9866 - val_loss: 0.4303 - val_rmse: 0.4111 - val_r_square: 0.9983\n",
      "Epoch 68/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4804 - rmse: 1.0319 - r_square: 0.9864 - val_loss: 0.4161 - val_rmse: 0.4053 - val_r_square: 0.9984\n",
      "Epoch 69/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4885 - rmse: 1.0332 - r_square: 0.9864 - val_loss: 0.4140 - val_rmse: 0.3894 - val_r_square: 0.9984\n",
      "Epoch 70/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5527 - rmse: 1.0432 - r_square: 0.9861 - val_loss: 0.6207 - val_rmse: 0.4584 - val_r_square: 0.9976\n",
      "Epoch 71/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.4737 - rmse: 1.0318 - r_square: 0.9864 - val_loss: 0.5385 - val_rmse: 0.4219 - val_r_square: 0.9979\n",
      "Epoch 72/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5108 - rmse: 1.0369 - r_square: 0.9863 - val_loss: 0.4410 - val_rmse: 0.4013 - val_r_square: 0.9983\n",
      "Epoch 73/100\n",
      "863855/863855 [==============================] - 5s 6us/step - loss: 3.5535 - rmse: 1.0400 - r_square: 0.9861 - val_loss: 0.4111 - val_rmse: 0.4086 - val_r_square: 0.9984\n",
      "Epoch 00073: early stopping\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential([\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "model3.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")\n",
    "# fit the model\n",
    "result3 = model3.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 100,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8519847 ]\n",
      " [ 7.147746  ]\n",
      " [44.128983  ]\n",
      " [12.572897  ]\n",
      " [ 2.321763  ]\n",
      " [ 0.10606444]\n",
      " [ 6.0779285 ]\n",
      " [ 4.6703777 ]\n",
      " [ 8.171738  ]\n",
      " [17.301027  ]]\n",
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.406705\n",
      "Mean squared error (MSE):       0.405651\n",
      "Root mean squared error (RMSE): 0.636908\n",
      "R square (R^2):                 0.998464\n"
     ]
    }
   ],
   "source": [
    "predictions3 = model3.predict(X_test)\n",
    "print(predictions3[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions3))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions3))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions3)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: \n",
    "&emsp;&emsp;What does 'Accuracy' mean in Regression? https://github.com/keras-team/keras/issues/7947 <br>\n",
    "&emsp;&emsp;https://keras.io/metrics/<br>\n",
    "&emsp;&emsp;Scale, Standardize, or Normalize with Scikit-Learn https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02 <br>\n",
    "&emsp;&emsp;The Day my Computer Won the Nobel Prize (Neural Network Option Pricing)  https://medium.com/datadriveninvestor/the-day-my-computer-won-the-nobel-prize-neural-network-option-pricing-d29b4379f1d2 <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
