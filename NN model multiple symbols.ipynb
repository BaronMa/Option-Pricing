{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-19.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Requirement already satisfied: h5py in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: pyyaml in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.14.3)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files\\anaconda3\\lib\\site-packages (from keras) (1.11.0)\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/4a/5c86ed8b245aa48f9f819b13a0a9039e9126ba19fdd0c7e0b8026c12315a/tensorflow-1.14.0-cp36-cp36m-win_amd64.whl (68.3MB)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/59/ec6c6075dd628f2efd33dbe11b259e63e1de7cbc26e45b38de767df528d4/protobuf-3.8.0-cp36-cp36m-win_amd64.whl (1.1MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.1.0)\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/6a/e83233ed636bdf8668f0e79897fd70bce04869482dd88f3cfc4c42404fb2/grpcio-1.21.1-cp36-cp36m-win_amd64.whl (1.6MB)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.1)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/ed/e036d31a9b2c750f270cbb1cfc1c0f94ac78ae504eea7eec3267be4e294a/numpy-1.16.4-cp36-cp36m-win_amd64.whl (11.9MB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n",
      "Requirement already satisfied: h5py in c:\\program files\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.1)\n",
      "Building wheels for collected packages: gast, absl-py, wrapt, termcolor\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\ee\\98\\38\\46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\d7\\de\\2e\\efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mayingzh\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built gast absl-py wrapt termcolor\n",
      "Installing collected packages: gast, setuptools, markdown, absl-py, numpy, grpcio, protobuf, tensorboard, wrapt, google-pasta, termcolor, astor, tensorflow-estimator, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.21.1 markdown-3.1.1 numpy-1.16.4 protobuf-3.8.0 setuptools-41.0.1 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 wrapt-1.11.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts freeze_graph.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in c:\\users\\mayingzh\\appdata\\roaming\\python\\python36\\site-packages (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user keras\n",
    "!pip install --user tensorflow\n",
    "!pip install --user -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU only\n",
    "# Only Macbook needs to run this cell\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and do data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>exdate</th>\n",
       "      <th>maturity</th>\n",
       "      <th>strike_price</th>\n",
       "      <th>best_offer</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>underlying_price</th>\n",
       "      <th>cp_flag_C</th>\n",
       "      <th>cp_flag_P</th>\n",
       "      <th>interest_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.181376</td>\n",
       "      <td>58.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>62.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.450289</td>\n",
       "      <td>58.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>58.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>67.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.369425</td>\n",
       "      <td>58.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AXP</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.888123</td>\n",
       "      <td>58.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date      exdate  maturity  strike_price  best_offer  \\\n",
       "0    AXP  2013-01-02  2013-01-04         2          60.0        0.03   \n",
       "1    AXP  2013-01-02  2013-01-04         2          62.5        0.05   \n",
       "2    AXP  2013-01-02  2013-01-04         2          65.0        0.05   \n",
       "3    AXP  2013-01-02  2013-01-04         2          67.5        0.50   \n",
       "4    AXP  2013-01-02  2013-01-04         2          70.0        0.01   \n",
       "\n",
       "   impl_volatility  underlying_price  cp_flag_C  cp_flag_P  interest_rate  \n",
       "0         0.181376             58.75          1          0           0.75  \n",
       "1         0.450289             58.75          1          0           0.75  \n",
       "2         0.676564             58.75          1          0           0.75  \n",
       "3         1.369425             58.75          1          0           0.75  \n",
       "4         0.888123             58.75          1          0           0.75  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Options.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['best_offer'].values\n",
    "X = df[['maturity', 'strike_price', 'impl_volatility', 'underlying_price', 'cp_flag_C', 'cp_flag_P', 'interest_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to build a regression neural network model\n",
    "sc= preprocessing.MinMaxScaler()\n",
    "X= sc.fit_transform(X)\n",
    "y= y.reshape(-1,1)\n",
    "y=sc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573351, 7)\n",
      "(1573351, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train, XX_validation, yy_train, yy_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a keras Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0623 22:05:10.809075  8672 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0623 22:05:10.824078  8672 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0623 22:05:10.831076  8672 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Built Keras sequential model with 3 hidden layer, and after the first hidden layer.\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable early stopping based on the loss of validation data\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because it is a simple regression problem, we should custom metrics function\n",
    "from keras import backend as K\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis = -1))\n",
    "\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 22:05:29.352179  8672 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use mean_squared_error to compile regression model loss\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 22:05:38.106596  8672 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0623 22:05:38.246649  8672 deprecation_wrapper.py:119] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1258680 samples, validate on 314671 samples\n",
      "Epoch 1/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 1.3830e-04 - rmse: 0.0043 - r_square: 0.9870 - val_loss: 1.5318e-05 - val_rmse: 0.0029 - val_r_square: 0.9985\n",
      "Epoch 2/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 1.4052e-05 - rmse: 0.0026 - r_square: 0.9986 - val_loss: 8.2428e-06 - val_rmse: 0.0020 - val_r_square: 0.9992\n",
      "Epoch 3/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 1.1143e-05 - rmse: 0.0023 - r_square: 0.9989 - val_loss: 1.2821e-05 - val_rmse: 0.0025 - val_r_square: 0.9987\n",
      "Epoch 4/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 9.3023e-06 - rmse: 0.0020 - r_square: 0.9991 - val_loss: 7.1806e-06 - val_rmse: 0.0018 - val_r_square: 0.9993\n",
      "Epoch 5/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 8.2399e-06 - rmse: 0.0018 - r_square: 0.9992 - val_loss: 6.4446e-06 - val_rmse: 0.0017 - val_r_square: 0.9994\n",
      "Epoch 6/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 7.5815e-06 - rmse: 0.0017 - r_square: 0.9993 - val_loss: 7.0741e-06 - val_rmse: 0.0016 - val_r_square: 0.9993\n",
      "Epoch 7/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 7.0245e-06 - rmse: 0.0017 - r_square: 0.9993 - val_loss: 5.8547e-06 - val_rmse: 0.0015 - val_r_square: 0.9994\n",
      "Epoch 8/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 6.7917e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 5.3086e-06 - val_rmse: 0.0014 - val_r_square: 0.9995\n",
      "Epoch 9/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 6.5593e-06 - rmse: 0.0016 - r_square: 0.9994 - val_loss: 5.8487e-06 - val_rmse: 0.0015 - val_r_square: 0.9994\n",
      "Epoch 10/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 6.4477e-06 - rmse: 0.0016 - r_square: 0.9994 - val_loss: 5.2027e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 11/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 6.1605e-06 - rmse: 0.0015 - r_square: 0.9994 - val_loss: 7.3021e-06 - val_rmse: 0.0017 - val_r_square: 0.9993\n",
      "Epoch 12/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 6.0881e-06 - rmse: 0.0015 - r_square: 0.9994 - val_loss: 7.5672e-06 - val_rmse: 0.0022 - val_r_square: 0.9993\n",
      "Epoch 13/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.9101e-06 - rmse: 0.0015 - r_square: 0.9994 - val_loss: 7.6020e-06 - val_rmse: 0.0018 - val_r_square: 0.9993\n",
      "Epoch 14/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.8849e-06 - rmse: 0.0015 - r_square: 0.9994 - val_loss: 5.0843e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 15/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.6989e-06 - rmse: 0.0014 - r_square: 0.9994 - val_loss: 4.9752e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 16/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.6549e-06 - rmse: 0.0014 - r_square: 0.9994 - val_loss: 4.6705e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 17/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.6098e-06 - rmse: 0.0014 - r_square: 0.9994 - val_loss: 4.8365e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 18/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.4889e-06 - rmse: 0.0014 - r_square: 0.9995 - val_loss: 5.7837e-06 - val_rmse: 0.0016 - val_r_square: 0.9994\n",
      "Epoch 19/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.4277e-06 - rmse: 0.0014 - r_square: 0.9995 - val_loss: 4.7729e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 20/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.3005e-06 - rmse: 0.0014 - r_square: 0.9995 - val_loss: 5.6423e-06 - val_rmse: 0.0014 - val_r_square: 0.9994\n",
      "Epoch 21/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.3339e-06 - rmse: 0.0014 - r_square: 0.9995 - val_loss: 6.3199e-06 - val_rmse: 0.0015 - val_r_square: 0.9994\n",
      "Epoch 22/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.3248e-06 - rmse: 0.0014 - r_square: 0.9995 - val_loss: 5.1657e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 23/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.2445e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.2241e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 24/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.2015e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.5272e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 25/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.2739e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.6517e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 26/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.1463e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.4074e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 27/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.1302e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.6434e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 28/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.1065e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.6659e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 29/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.1017e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.2824e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 30/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.0302e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.3973e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 31/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 5.0237e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.2078e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 32/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.9317e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.6709e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 33/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.9471e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.6881e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 34/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.9432e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.2731e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 35/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.9192e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.2928e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 36/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.9129e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 5.4086e-06 - val_rmse: 0.0014 - val_r_square: 0.9995\n",
      "Epoch 37/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.8564e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.2561e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 38/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.8253e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.9248e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 39/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.8164e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.4204e-06 - val_rmse: 0.0013 - val_r_square: 0.9996\n",
      "Epoch 40/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.8688e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 5.0159e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 41/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.7406e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.5980e-06 - val_rmse: 0.0013 - val_r_square: 0.9996\n",
      "Epoch 42/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.7614e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 5.6115e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.7693e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 4.5705e-06 - val_rmse: 0.0013 - val_r_square: 0.9996\n",
      "Epoch 44/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.7890e-06 - rmse: 0.0013 - r_square: 0.9995 - val_loss: 7.8233e-06 - val_rmse: 0.0017 - val_r_square: 0.9992\n",
      "Epoch 45/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.7232e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.4057e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 46/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.7465e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.0438e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 47/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6876e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 5.5125e-06 - val_rmse: 0.0015 - val_r_square: 0.9995\n",
      "Epoch 48/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.7332e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.4485e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 49/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6789e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.8045e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 50/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6561e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 5.8879e-06 - val_rmse: 0.0015 - val_r_square: 0.9994\n",
      "Epoch 51/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6708e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.3479e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 52/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6292e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 5.0305e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 53/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6505e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 3.9805e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 54/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6119e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 5.4690e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 55/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6480e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.7839e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 56/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6020e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 3.8749e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 57/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.6077e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 3.8715e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 58/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5913e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.4839e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 59/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5658e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.8176e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 60/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5716e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 5.0350e-06 - val_rmse: 0.0014 - val_r_square: 0.9995\n",
      "Epoch 61/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5417e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 5.2228e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 62/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5472e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.3362e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 63/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5294e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.2097e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 64/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5394e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 7.5857e-06 - val_rmse: 0.0016 - val_r_square: 0.9993\n",
      "Epoch 65/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5831e-06 - rmse: 0.0012 - r_square: 0.9995 - val_loss: 4.2468e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 66/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5130e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 6.7138e-06 - val_rmse: 0.0015 - val_r_square: 0.9993\n",
      "Epoch 67/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5273e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 5.4828e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 68/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4996e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 3.8365e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 69/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5000e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.0412e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 70/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4990e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.7541e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 71/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4552e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.1436e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 72/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.5349e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 3.9243e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 73/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4634e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.8535e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 74/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4632e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.0523e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 75/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4790e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.5094e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 76/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4539e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.1204e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 77/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4598e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.5151e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 78/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4120e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 3.9206e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 79/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4452e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.8260e-06 - val_rmse: 0.0012 - val_r_square: 0.9995\n",
      "Epoch 80/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4519e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.9273e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 81/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4214e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 3.8972e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 82/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4172e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.1564e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 83/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.3549e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 3.9449e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 84/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4025e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.1850e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4064e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 6.3234e-06 - val_rmse: 0.0014 - val_r_square: 0.9994\n",
      "Epoch 86/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.3737e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 5.4009e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 87/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.4058e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.6583e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 88/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.3975e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 5.2906e-06 - val_rmse: 0.0013 - val_r_square: 0.9995\n",
      "Epoch 89/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.3923e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.0675e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 90/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.3622e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.0420e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 91/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.3588e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.2267e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 92/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.3606e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.0422e-06 - val_rmse: 0.0011 - val_r_square: 0.9996\n",
      "Epoch 93/200\n",
      "1258680/1258680 [==============================] - 6s 5us/step - loss: 4.3688e-06 - rmse: 0.0012 - r_square: 0.9996 - val_loss: 4.5965e-06 - val_rmse: 0.0012 - val_r_square: 0.9996\n",
      "Epoch 00093: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "result = model.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 200,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05547181]\n",
      " [0.00232244]\n",
      " [0.00163608]\n",
      " [0.08015914]\n",
      " [0.02986703]\n",
      " [0.000429  ]\n",
      " [0.00038146]\n",
      " [0.11281534]\n",
      " [0.00167163]\n",
      " [0.00037305]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05267722],\n",
       "       [0.00354072],\n",
       "       [0.00187875],\n",
       "       [0.07471638],\n",
       "       [0.03136065],\n",
       "       [0.0003613 ],\n",
       "       [0.00028904],\n",
       "       [0.11337524],\n",
       "       [0.00122841],\n",
       "       [0.00050582]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curves including R^2 and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training curve for rmse\n",
    "plt.plot(result.history['rmse'])\n",
    "plt.plot(result.history['val_rmse'])\n",
    "plt.title('rmse')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training curve for R^2\n",
    "plt.plot(result.history['r_square'])\n",
    "plt.plot(result.history['val_r_square'])\n",
    "plt.title('model R^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.001187\n",
      "Mean squared error (MSE):       0.000005\n",
      "Root mean squared error (RMSE): 0.002136\n",
      "R square (R^2):                 0.999570\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as skm, math\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 22:18:30.824930  8672 deprecation.py:506] From C:\\Users\\mayingzh\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1258680 samples, validate on 314671 samples\n",
      "Epoch 1/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 3.1370e-04 - rmse: 0.0083 - r_square: 0.9698 - val_loss: 1.7105e-04 - val_rmse: 0.0069 - val_r_square: 0.9840\n",
      "Epoch 2/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 3.1086e-05 - rmse: 0.0035 - r_square: 0.9970 - val_loss: 2.2764e-04 - val_rmse: 0.0075 - val_r_square: 0.9787\n",
      "Epoch 3/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 2.0675e-05 - rmse: 0.0028 - r_square: 0.9980 - val_loss: 1.7900e-04 - val_rmse: 0.0068 - val_r_square: 0.9833\n",
      "Epoch 4/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.6610e-05 - rmse: 0.0025 - r_square: 0.9984 - val_loss: 1.5037e-04 - val_rmse: 0.0072 - val_r_square: 0.9859\n",
      "Epoch 5/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.4679e-05 - rmse: 0.0023 - r_square: 0.9986 - val_loss: 1.6876e-04 - val_rmse: 0.0074 - val_r_square: 0.9841\n",
      "Epoch 6/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3298e-05 - rmse: 0.0022 - r_square: 0.9987 - val_loss: 1.5917e-04 - val_rmse: 0.0071 - val_r_square: 0.9851\n",
      "Epoch 7/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2137e-05 - rmse: 0.0021 - r_square: 0.9988 - val_loss: 1.5118e-04 - val_rmse: 0.0077 - val_r_square: 0.9858\n",
      "Epoch 8/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.1786e-05 - rmse: 0.0021 - r_square: 0.9988 - val_loss: 1.8885e-04 - val_rmse: 0.0084 - val_r_square: 0.9822\n",
      "Epoch 9/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.1052e-05 - rmse: 0.0020 - r_square: 0.9989 - val_loss: 1.7274e-04 - val_rmse: 0.0080 - val_r_square: 0.9838\n",
      "Epoch 10/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.0704e-05 - rmse: 0.0020 - r_square: 0.9989 - val_loss: 1.8405e-04 - val_rmse: 0.0082 - val_r_square: 0.9827\n",
      "Epoch 11/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.0338e-05 - rmse: 0.0019 - r_square: 0.9990 - val_loss: 1.7627e-04 - val_rmse: 0.0085 - val_r_square: 0.9834\n",
      "Epoch 12/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.0068e-05 - rmse: 0.0019 - r_square: 0.9990 - val_loss: 1.4963e-04 - val_rmse: 0.0076 - val_r_square: 0.9859\n",
      "Epoch 13/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 9.6855e-06 - rmse: 0.0019 - r_square: 0.9990 - val_loss: 1.4320e-04 - val_rmse: 0.0076 - val_r_square: 0.9865\n",
      "Epoch 14/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 9.3663e-06 - rmse: 0.0018 - r_square: 0.9991 - val_loss: 1.7815e-04 - val_rmse: 0.0077 - val_r_square: 0.9833\n",
      "Epoch 15/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 9.4099e-06 - rmse: 0.0018 - r_square: 0.9991 - val_loss: 1.6877e-04 - val_rmse: 0.0080 - val_r_square: 0.9842\n",
      "Epoch 16/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 9.1729e-06 - rmse: 0.0018 - r_square: 0.9991 - val_loss: 1.6786e-04 - val_rmse: 0.0083 - val_r_square: 0.9842\n",
      "Epoch 17/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 9.0570e-06 - rmse: 0.0018 - r_square: 0.9991 - val_loss: 1.5845e-04 - val_rmse: 0.0071 - val_r_square: 0.9852\n",
      "Epoch 18/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 8.8429e-06 - rmse: 0.0018 - r_square: 0.9991 - val_loss: 1.7300e-04 - val_rmse: 0.0078 - val_r_square: 0.9837\n",
      "Epoch 19/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 8.6511e-06 - rmse: 0.0018 - r_square: 0.9991 - val_loss: 1.6629e-04 - val_rmse: 0.0077 - val_r_square: 0.9844\n",
      "Epoch 20/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 8.5269e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.6536e-04 - val_rmse: 0.0077 - val_r_square: 0.9845\n",
      "Epoch 21/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 8.4399e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.7210e-04 - val_rmse: 0.0076 - val_r_square: 0.9839\n",
      "Epoch 22/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 8.3649e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.7426e-04 - val_rmse: 0.0079 - val_r_square: 0.9837\n",
      "Epoch 23/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 8.1749e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.5536e-04 - val_rmse: 0.0074 - val_r_square: 0.9854\n",
      "Epoch 24/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 8.0317e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.5397e-04 - val_rmse: 0.0072 - val_r_square: 0.9856\n",
      "Epoch 25/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 8.1812e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.6374e-04 - val_rmse: 0.0076 - val_r_square: 0.9847\n",
      "Epoch 26/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.9183e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.5851e-04 - val_rmse: 0.0075 - val_r_square: 0.9852\n",
      "Epoch 27/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.7656e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.4722e-04 - val_rmse: 0.0076 - val_r_square: 0.9862\n",
      "Epoch 28/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.8176e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.6618e-04 - val_rmse: 0.0074 - val_r_square: 0.9845\n",
      "Epoch 29/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.6225e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.6610e-04 - val_rmse: 0.0077 - val_r_square: 0.9844\n",
      "Epoch 30/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.8187e-06 - rmse: 0.0017 - r_square: 0.9992 - val_loss: 1.5477e-04 - val_rmse: 0.0075 - val_r_square: 0.9855\n",
      "Epoch 31/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.6164e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.8672e-04 - val_rmse: 0.0079 - val_r_square: 0.9825\n",
      "Epoch 32/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.5154e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.7772e-04 - val_rmse: 0.0079 - val_r_square: 0.9833\n",
      "Epoch 33/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.4642e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.6863e-04 - val_rmse: 0.0078 - val_r_square: 0.9842\n",
      "Epoch 34/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.4859e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.6790e-04 - val_rmse: 0.0079 - val_r_square: 0.9843\n",
      "Epoch 35/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.3232e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.6169e-04 - val_rmse: 0.0072 - val_r_square: 0.9849\n",
      "Epoch 36/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.3555e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.6314e-04 - val_rmse: 0.0076 - val_r_square: 0.9847\n",
      "Epoch 37/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.2178e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.5302e-04 - val_rmse: 0.0075 - val_r_square: 0.9856\n",
      "Epoch 38/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 7.1247e-06 - rmse: 0.0016 - r_square: 0.9993 - val_loss: 1.6302e-04 - val_rmse: 0.0074 - val_r_square: 0.9848\n",
      "Epoch 00038: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dropout(0.1),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "model2.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")\n",
    "# fit the model\n",
    "result2 = model2.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 200,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04707419]\n",
      " [0.01022818]\n",
      " [0.00748053]\n",
      " [0.06863838]\n",
      " [0.034431  ]\n",
      " [0.00056269]\n",
      " [0.0017158 ]\n",
      " [0.1073888 ]\n",
      " [0.00314184]\n",
      " [0.0028066 ]]\n",
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.007355\n",
      "Mean squared error (MSE):       0.000160\n",
      "Root mean squared error (RMSE): 0.012660\n",
      "R square (R^2):                 0.984910\n"
     ]
    }
   ],
   "source": [
    "predictions2 = model2.predict(X_test)\n",
    "print(predictions2[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions2))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions2))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions2)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try batch normalization technique to regularize neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1258680 samples, validate on 314671 samples\n",
      "Epoch 1/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 3.0125e-04 - rmse: 0.0102 - r_square: 0.9709 - val_loss: 1.1729e-04 - val_rmse: 0.0057 - val_r_square: 0.9887\n",
      "Epoch 2/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.6909e-04 - rmse: 0.0074 - r_square: 0.9837 - val_loss: 3.1365e-05 - val_rmse: 0.0039 - val_r_square: 0.9969\n",
      "Epoch 3/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.6013e-04 - rmse: 0.0069 - r_square: 0.9845 - val_loss: 2.0933e-05 - val_rmse: 0.0028 - val_r_square: 0.9980\n",
      "Epoch 4/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.4954e-04 - rmse: 0.0066 - r_square: 0.9855 - val_loss: 2.8022e-05 - val_rmse: 0.0030 - val_r_square: 0.9973\n",
      "Epoch 5/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.4935e-04 - rmse: 0.0065 - r_square: 0.9855 - val_loss: 3.0421e-05 - val_rmse: 0.0032 - val_r_square: 0.9970\n",
      "Epoch 6/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.4709e-04 - rmse: 0.0065 - r_square: 0.9858 - val_loss: 4.1387e-05 - val_rmse: 0.0039 - val_r_square: 0.9960\n",
      "Epoch 7/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.4219e-04 - rmse: 0.0064 - r_square: 0.9863 - val_loss: 8.3608e-05 - val_rmse: 0.0051 - val_r_square: 0.9920\n",
      "Epoch 8/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.4514e-04 - rmse: 0.0064 - r_square: 0.9859 - val_loss: 2.3281e-05 - val_rmse: 0.0029 - val_r_square: 0.9977\n",
      "Epoch 9/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3992e-04 - rmse: 0.0063 - r_square: 0.9865 - val_loss: 3.2823e-05 - val_rmse: 0.0032 - val_r_square: 0.9968\n",
      "Epoch 10/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.4228e-04 - rmse: 0.0064 - r_square: 0.9863 - val_loss: 1.8509e-05 - val_rmse: 0.0026 - val_r_square: 0.9982\n",
      "Epoch 11/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.4174e-04 - rmse: 0.0063 - r_square: 0.9863 - val_loss: 3.0937e-05 - val_rmse: 0.0033 - val_r_square: 0.9970\n",
      "Epoch 12/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.4086e-04 - rmse: 0.0063 - r_square: 0.9863 - val_loss: 2.2080e-05 - val_rmse: 0.0034 - val_r_square: 0.9978\n",
      "Epoch 13/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.4203e-04 - rmse: 0.0063 - r_square: 0.9863 - val_loss: 3.3267e-05 - val_rmse: 0.0034 - val_r_square: 0.9968\n",
      "Epoch 14/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3935e-04 - rmse: 0.0063 - r_square: 0.9865 - val_loss: 1.8664e-05 - val_rmse: 0.0027 - val_r_square: 0.9982\n",
      "Epoch 15/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3778e-04 - rmse: 0.0063 - r_square: 0.9867 - val_loss: 1.8917e-05 - val_rmse: 0.0032 - val_r_square: 0.9982\n",
      "Epoch 16/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3812e-04 - rmse: 0.0063 - r_square: 0.9866 - val_loss: 2.4670e-05 - val_rmse: 0.0026 - val_r_square: 0.9976\n",
      "Epoch 17/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3942e-04 - rmse: 0.0063 - r_square: 0.9865 - val_loss: 1.8929e-05 - val_rmse: 0.0027 - val_r_square: 0.9982\n",
      "Epoch 18/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3710e-04 - rmse: 0.0062 - r_square: 0.9867 - val_loss: 2.4288e-05 - val_rmse: 0.0037 - val_r_square: 0.9976\n",
      "Epoch 19/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3849e-04 - rmse: 0.0063 - r_square: 0.9866 - val_loss: 3.5699e-05 - val_rmse: 0.0035 - val_r_square: 0.9966\n",
      "Epoch 20/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3989e-04 - rmse: 0.0063 - r_square: 0.9865 - val_loss: 2.1765e-05 - val_rmse: 0.0028 - val_r_square: 0.9979\n",
      "Epoch 21/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3593e-04 - rmse: 0.0062 - r_square: 0.9869 - val_loss: 2.4273e-05 - val_rmse: 0.0033 - val_r_square: 0.9977\n",
      "Epoch 22/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3852e-04 - rmse: 0.0063 - r_square: 0.9864 - val_loss: 2.4437e-05 - val_rmse: 0.0026 - val_r_square: 0.9977\n",
      "Epoch 23/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3480e-04 - rmse: 0.0062 - r_square: 0.9870 - val_loss: 2.2184e-05 - val_rmse: 0.0032 - val_r_square: 0.9979\n",
      "Epoch 24/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3472e-04 - rmse: 0.0062 - r_square: 0.9870 - val_loss: 2.6944e-05 - val_rmse: 0.0030 - val_r_square: 0.9974\n",
      "Epoch 25/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3358e-04 - rmse: 0.0062 - r_square: 0.9870 - val_loss: 1.9323e-05 - val_rmse: 0.0029 - val_r_square: 0.9981\n",
      "Epoch 26/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3180e-04 - rmse: 0.0062 - r_square: 0.9873 - val_loss: 2.3177e-05 - val_rmse: 0.0029 - val_r_square: 0.9978\n",
      "Epoch 27/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3357e-04 - rmse: 0.0062 - r_square: 0.9871 - val_loss: 4.9947e-05 - val_rmse: 0.0047 - val_r_square: 0.9952\n",
      "Epoch 28/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3953e-04 - rmse: 0.0063 - r_square: 0.9865 - val_loss: 2.9635e-05 - val_rmse: 0.0039 - val_r_square: 0.9971\n",
      "Epoch 29/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3568e-04 - rmse: 0.0062 - r_square: 0.9869 - val_loss: 2.3745e-05 - val_rmse: 0.0029 - val_r_square: 0.9977\n",
      "Epoch 30/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3456e-04 - rmse: 0.0062 - r_square: 0.9870 - val_loss: 1.9288e-05 - val_rmse: 0.0024 - val_r_square: 0.9981\n",
      "Epoch 31/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3074e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 1.9264e-05 - val_rmse: 0.0023 - val_r_square: 0.9982\n",
      "Epoch 32/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3537e-04 - rmse: 0.0062 - r_square: 0.9868 - val_loss: 2.4918e-05 - val_rmse: 0.0028 - val_r_square: 0.9976\n",
      "Epoch 33/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3375e-04 - rmse: 0.0062 - r_square: 0.9871 - val_loss: 3.2888e-05 - val_rmse: 0.0036 - val_r_square: 0.9968\n",
      "Epoch 34/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3797e-04 - rmse: 0.0063 - r_square: 0.9866 - val_loss: 5.8677e-05 - val_rmse: 0.0038 - val_r_square: 0.9944\n",
      "Epoch 35/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3701e-04 - rmse: 0.0062 - r_square: 0.9867 - val_loss: 1.8290e-05 - val_rmse: 0.0024 - val_r_square: 0.9982\n",
      "Epoch 36/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2771e-04 - rmse: 0.0060 - r_square: 0.9877 - val_loss: 2.2518e-05 - val_rmse: 0.0026 - val_r_square: 0.9978\n",
      "Epoch 37/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3213e-04 - rmse: 0.0061 - r_square: 0.9872 - val_loss: 4.1022e-05 - val_rmse: 0.0040 - val_r_square: 0.9961\n",
      "Epoch 38/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3569e-04 - rmse: 0.0062 - r_square: 0.9869 - val_loss: 2.6875e-05 - val_rmse: 0.0032 - val_r_square: 0.9974\n",
      "Epoch 39/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3425e-04 - rmse: 0.0061 - r_square: 0.9870 - val_loss: 2.5631e-05 - val_rmse: 0.0032 - val_r_square: 0.9975\n",
      "Epoch 40/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3108e-04 - rmse: 0.0061 - r_square: 0.9872 - val_loss: 2.1252e-05 - val_rmse: 0.0032 - val_r_square: 0.9979\n",
      "Epoch 41/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3282e-04 - rmse: 0.0061 - r_square: 0.9872 - val_loss: 5.5856e-05 - val_rmse: 0.0039 - val_r_square: 0.9946\n",
      "Epoch 42/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3139e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 2.4314e-05 - val_rmse: 0.0032 - val_r_square: 0.9976\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3083e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 2.0409e-05 - val_rmse: 0.0026 - val_r_square: 0.9981\n",
      "Epoch 44/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3274e-04 - rmse: 0.0061 - r_square: 0.9871 - val_loss: 2.4348e-05 - val_rmse: 0.0028 - val_r_square: 0.9977\n",
      "Epoch 45/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3456e-04 - rmse: 0.0062 - r_square: 0.9870 - val_loss: 2.2158e-05 - val_rmse: 0.0029 - val_r_square: 0.9979\n",
      "Epoch 46/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3416e-04 - rmse: 0.0062 - r_square: 0.9871 - val_loss: 1.6966e-05 - val_rmse: 0.0023 - val_r_square: 0.9984\n",
      "Epoch 47/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3150e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 2.1001e-05 - val_rmse: 0.0026 - val_r_square: 0.9980\n",
      "Epoch 48/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3304e-04 - rmse: 0.0061 - r_square: 0.9872 - val_loss: 2.5915e-05 - val_rmse: 0.0039 - val_r_square: 0.9975\n",
      "Epoch 49/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3151e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 3.1087e-05 - val_rmse: 0.0036 - val_r_square: 0.9970\n",
      "Epoch 50/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3228e-04 - rmse: 0.0061 - r_square: 0.9872 - val_loss: 2.2862e-05 - val_rmse: 0.0026 - val_r_square: 0.9978\n",
      "Epoch 51/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3256e-04 - rmse: 0.0061 - r_square: 0.9871 - val_loss: 2.7558e-05 - val_rmse: 0.0033 - val_r_square: 0.9973\n",
      "Epoch 52/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3401e-04 - rmse: 0.0062 - r_square: 0.9871 - val_loss: 2.7921e-05 - val_rmse: 0.0032 - val_r_square: 0.9973\n",
      "Epoch 53/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3080e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 2.3359e-05 - val_rmse: 0.0034 - val_r_square: 0.9977\n",
      "Epoch 54/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3071e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 2.2213e-05 - val_rmse: 0.0027 - val_r_square: 0.9979\n",
      "Epoch 55/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3097e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 3.5198e-05 - val_rmse: 0.0033 - val_r_square: 0.9967\n",
      "Epoch 56/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3155e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 2.2309e-05 - val_rmse: 0.0034 - val_r_square: 0.9978\n",
      "Epoch 57/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3153e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 2.0312e-05 - val_rmse: 0.0027 - val_r_square: 0.9980\n",
      "Epoch 58/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2853e-04 - rmse: 0.0060 - r_square: 0.9876 - val_loss: 2.5855e-05 - val_rmse: 0.0030 - val_r_square: 0.9975\n",
      "Epoch 59/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3123e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 2.8746e-05 - val_rmse: 0.0037 - val_r_square: 0.9972\n",
      "Epoch 60/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2527e-04 - rmse: 0.0060 - r_square: 0.9878 - val_loss: 1.9681e-05 - val_rmse: 0.0026 - val_r_square: 0.9981\n",
      "Epoch 61/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3145e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 2.6749e-05 - val_rmse: 0.0028 - val_r_square: 0.9974\n",
      "Epoch 62/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3057e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 2.3822e-05 - val_rmse: 0.0035 - val_r_square: 0.9977\n",
      "Epoch 63/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3177e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 1.5685e-05 - val_rmse: 0.0024 - val_r_square: 0.9985\n",
      "Epoch 64/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3075e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.7657e-05 - val_rmse: 0.0025 - val_r_square: 0.9983\n",
      "Epoch 65/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2861e-04 - rmse: 0.0061 - r_square: 0.9875 - val_loss: 1.9753e-05 - val_rmse: 0.0028 - val_r_square: 0.9981\n",
      "Epoch 66/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3056e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.8348e-05 - val_rmse: 0.0030 - val_r_square: 0.9982\n",
      "Epoch 67/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2878e-04 - rmse: 0.0061 - r_square: 0.9875 - val_loss: 2.3655e-05 - val_rmse: 0.0034 - val_r_square: 0.9977\n",
      "Epoch 68/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3345e-04 - rmse: 0.0061 - r_square: 0.9871 - val_loss: 2.5966e-05 - val_rmse: 0.0029 - val_r_square: 0.9975\n",
      "Epoch 69/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3032e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.9687e-05 - val_rmse: 0.0027 - val_r_square: 0.9981\n",
      "Epoch 70/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2877e-04 - rmse: 0.0060 - r_square: 0.9875 - val_loss: 2.0918e-05 - val_rmse: 0.0031 - val_r_square: 0.9980\n",
      "Epoch 71/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2808e-04 - rmse: 0.0060 - r_square: 0.9876 - val_loss: 1.7010e-05 - val_rmse: 0.0026 - val_r_square: 0.9983\n",
      "Epoch 72/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2908e-04 - rmse: 0.0061 - r_square: 0.9876 - val_loss: 2.3047e-05 - val_rmse: 0.0027 - val_r_square: 0.9978\n",
      "Epoch 73/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2630e-04 - rmse: 0.0060 - r_square: 0.9878 - val_loss: 2.8238e-05 - val_rmse: 0.0032 - val_r_square: 0.9973\n",
      "Epoch 74/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2813e-04 - rmse: 0.0060 - r_square: 0.9876 - val_loss: 1.9445e-05 - val_rmse: 0.0024 - val_r_square: 0.9981\n",
      "Epoch 75/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2923e-04 - rmse: 0.0060 - r_square: 0.9875 - val_loss: 1.7912e-05 - val_rmse: 0.0024 - val_r_square: 0.9983\n",
      "Epoch 76/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2974e-04 - rmse: 0.0060 - r_square: 0.9875 - val_loss: 1.7923e-05 - val_rmse: 0.0023 - val_r_square: 0.9983\n",
      "Epoch 77/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2703e-04 - rmse: 0.0060 - r_square: 0.9877 - val_loss: 1.7902e-05 - val_rmse: 0.0026 - val_r_square: 0.9983\n",
      "Epoch 78/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3279e-04 - rmse: 0.0061 - r_square: 0.9872 - val_loss: 1.7857e-05 - val_rmse: 0.0026 - val_r_square: 0.9983\n",
      "Epoch 79/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3160e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 6.8038e-05 - val_rmse: 0.0045 - val_r_square: 0.9935\n",
      "Epoch 80/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2914e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.6078e-05 - val_rmse: 0.0027 - val_r_square: 0.9984\n",
      "Epoch 81/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2871e-04 - rmse: 0.0060 - r_square: 0.9875 - val_loss: 2.6606e-05 - val_rmse: 0.0029 - val_r_square: 0.9974\n",
      "Epoch 82/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2982e-04 - rmse: 0.0060 - r_square: 0.9874 - val_loss: 4.0310e-05 - val_rmse: 0.0038 - val_r_square: 0.9961\n",
      "Epoch 83/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3014e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.8624e-05 - val_rmse: 0.0023 - val_r_square: 0.9982\n",
      "Epoch 84/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2988e-04 - rmse: 0.0060 - r_square: 0.9875 - val_loss: 1.4667e-05 - val_rmse: 0.0022 - val_r_square: 0.9986\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3067e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.5665e-05 - val_rmse: 0.0022 - val_r_square: 0.9985\n",
      "Epoch 86/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2999e-04 - rmse: 0.0061 - r_square: 0.9875 - val_loss: 2.0223e-05 - val_rmse: 0.0024 - val_r_square: 0.9981\n",
      "Epoch 87/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2935e-04 - rmse: 0.0061 - r_square: 0.9875 - val_loss: 2.7797e-05 - val_rmse: 0.0030 - val_r_square: 0.9973\n",
      "Epoch 88/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2934e-04 - rmse: 0.0060 - r_square: 0.9874 - val_loss: 1.9085e-05 - val_rmse: 0.0028 - val_r_square: 0.9981\n",
      "Epoch 89/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2984e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.6339e-05 - val_rmse: 0.0024 - val_r_square: 0.9984\n",
      "Epoch 90/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2932e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.9709e-05 - val_rmse: 0.0024 - val_r_square: 0.9981\n",
      "Epoch 91/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2862e-04 - rmse: 0.0060 - r_square: 0.9875 - val_loss: 1.4335e-05 - val_rmse: 0.0024 - val_r_square: 0.9986\n",
      "Epoch 92/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3177e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 1.8310e-05 - val_rmse: 0.0024 - val_r_square: 0.9982\n",
      "Epoch 93/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3000e-04 - rmse: 0.0061 - r_square: 0.9876 - val_loss: 1.7407e-05 - val_rmse: 0.0028 - val_r_square: 0.9983\n",
      "Epoch 94/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2954e-04 - rmse: 0.0061 - r_square: 0.9875 - val_loss: 2.1303e-05 - val_rmse: 0.0025 - val_r_square: 0.9979\n",
      "Epoch 95/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2688e-04 - rmse: 0.0060 - r_square: 0.9877 - val_loss: 2.4820e-05 - val_rmse: 0.0036 - val_r_square: 0.9976\n",
      "Epoch 96/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2735e-04 - rmse: 0.0060 - r_square: 0.9877 - val_loss: 2.2415e-05 - val_rmse: 0.0033 - val_r_square: 0.9978\n",
      "Epoch 97/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.3151e-04 - rmse: 0.0061 - r_square: 0.9872 - val_loss: 3.0744e-05 - val_rmse: 0.0034 - val_r_square: 0.9970\n",
      "Epoch 98/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2922e-04 - rmse: 0.0060 - r_square: 0.9875 - val_loss: 3.8224e-05 - val_rmse: 0.0034 - val_r_square: 0.9963\n",
      "Epoch 99/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3004e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 2.0093e-05 - val_rmse: 0.0025 - val_r_square: 0.9981\n",
      "Epoch 100/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3093e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 2.3158e-05 - val_rmse: 0.0026 - val_r_square: 0.9978\n",
      "Epoch 101/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2693e-04 - rmse: 0.0060 - r_square: 0.9877 - val_loss: 2.8292e-05 - val_rmse: 0.0029 - val_r_square: 0.9973\n",
      "Epoch 102/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2741e-04 - rmse: 0.0060 - r_square: 0.9876 - val_loss: 1.9420e-05 - val_rmse: 0.0027 - val_r_square: 0.9981\n",
      "Epoch 103/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2615e-04 - rmse: 0.0060 - r_square: 0.9878 - val_loss: 1.7226e-05 - val_rmse: 0.0022 - val_r_square: 0.9984\n",
      "Epoch 104/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3067e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 1.7885e-05 - val_rmse: 0.0029 - val_r_square: 0.9983\n",
      "Epoch 105/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3382e-04 - rmse: 0.0061 - r_square: 0.9871 - val_loss: 1.8866e-05 - val_rmse: 0.0026 - val_r_square: 0.9982\n",
      "Epoch 106/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2959e-04 - rmse: 0.0060 - r_square: 0.9874 - val_loss: 2.1041e-05 - val_rmse: 0.0028 - val_r_square: 0.9980\n",
      "Epoch 107/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2707e-04 - rmse: 0.0060 - r_square: 0.9877 - val_loss: 3.0792e-05 - val_rmse: 0.0030 - val_r_square: 0.9970\n",
      "Epoch 108/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2938e-04 - rmse: 0.0061 - r_square: 0.9875 - val_loss: 5.2567e-05 - val_rmse: 0.0044 - val_r_square: 0.9949\n",
      "Epoch 109/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3053e-04 - rmse: 0.0061 - r_square: 0.9874 - val_loss: 3.0612e-05 - val_rmse: 0.0031 - val_r_square: 0.9971\n",
      "Epoch 110/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2891e-04 - rmse: 0.0060 - r_square: 0.9875 - val_loss: 1.6679e-05 - val_rmse: 0.0023 - val_r_square: 0.9984\n",
      "Epoch 111/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3199e-04 - rmse: 0.0061 - r_square: 0.9873 - val_loss: 2.5525e-05 - val_rmse: 0.0026 - val_r_square: 0.9975\n",
      "Epoch 112/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2773e-04 - rmse: 0.0060 - r_square: 0.9877 - val_loss: 2.2905e-05 - val_rmse: 0.0030 - val_r_square: 0.9978\n",
      "Epoch 113/200\n",
      "1258680/1258680 [==============================] - 7s 6us/step - loss: 1.2756e-04 - rmse: 0.0060 - r_square: 0.9877 - val_loss: 2.3498e-05 - val_rmse: 0.0030 - val_r_square: 0.9977\n",
      "Epoch 114/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2921e-04 - rmse: 0.0060 - r_square: 0.9876 - val_loss: 3.8060e-05 - val_rmse: 0.0036 - val_r_square: 0.9964\n",
      "Epoch 115/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.3345e-04 - rmse: 0.0061 - r_square: 0.9871 - val_loss: 2.4927e-05 - val_rmse: 0.0028 - val_r_square: 0.9976\n",
      "Epoch 116/200\n",
      "1258680/1258680 [==============================] - 7s 5us/step - loss: 1.2756e-04 - rmse: 0.0060 - r_square: 0.9876 - val_loss: 1.9144e-05 - val_rmse: 0.0025 - val_r_square: 0.9982\n",
      "Epoch 00116: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "model3 = Sequential([\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "model3.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[rmse, r_square],\n",
    ")\n",
    "# fit the model\n",
    "result3 = model3.fit(XX_train, \n",
    "                   yy_train,\n",
    "                   epochs = 200,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(XX_validation, yy_validation),\n",
    "                   callbacks = [es]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05443587]\n",
      " [0.00320067]\n",
      " [0.00082978]\n",
      " [0.0782204 ]\n",
      " [0.03396486]\n",
      " [0.00061673]\n",
      " [0.00061673]\n",
      " [0.1019396 ]\n",
      " [0.00114214]\n",
      " [0.00061673]]\n",
      "\n",
      "\n",
      "Mean absolute error (MAE):      0.002477\n",
      "Mean squared error (MSE):       0.000019\n",
      "Root mean squared error (RMSE): 0.004343\n",
      "R square (R^2):                 0.998224\n"
     ]
    }
   ],
   "source": [
    "predictions3 = model3.predict(X_test)\n",
    "print(predictions3[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % skm.mean_absolute_error(y_test,predictions3))\n",
    "print(\"Mean squared error (MSE):       %f\" % skm.mean_squared_error(y_test,predictions3))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(skm.mean_squared_error(y_test,predictions3)))\n",
    "print(\"R square (R^2):                 %f\" % skm.r2_score(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: \n",
    "&emsp;&emsp;What does 'Accuracy' mean in Regression? https://github.com/keras-team/keras/issues/7947 <br>\n",
    "&emsp;&emsp;https://keras.io/metrics/<br>\n",
    "&emsp;&emsp;Scale, Standardize, or Normalize with Scikit-Learn https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02 <br>\n",
    "&emsp;&emsp;The Day my Computer Won the Nobel Prize (Neural Network Option Pricing)  https://medium.com/datadriveninvestor/the-day-my-computer-won-the-nobel-prize-neural-network-option-pricing-d29b4379f1d2 <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
